{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a99b21-db3c-44bc-ba47-c72d4a4f235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca5b12a9-5bbc-4884-9edd-f0a75da71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(path, test=False):\n",
    "    #create appropriate file path\n",
    "    if test == False:\n",
    "        pfilename = path + \"/product_training.json\"\n",
    "        rfilename = path + \"/review_training.json\"\n",
    "    else:\n",
    "        pfilename = path + \"/product_test.json\"\n",
    "        rfilename = path + \"/review_test.json\"\n",
    "    \n",
    "    #extract files as pandas dataframes\n",
    "    product_df = pd.read_json(pfilename)\n",
    "    review_df = pd.read_json(rfilename).drop_duplicates(subset=[\"reviewerID\", \"unixReviewTime\"], keep=\"first\")\n",
    "    \n",
    "    review_df.drop(columns=[\"reviewerID\",\"vote\", \"unixReviewTime\",\"reviewTime\",\"style\",\"reviewerName\",\"image\"], axis=1 ,inplace=True)\n",
    "    \n",
    "    review_df['reviewText'].fillna(\"\", inplace=True)\n",
    "    review_df['summary'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    group = review_df.groupby(\"asin\")\n",
    "    \n",
    "    review_group_df = pd.DataFrame(columns = ['asin', 'numReviews', 'percentVerified','reviewText','summaryText'])\n",
    "    \n",
    "    for asin, data in group:\n",
    "        verifiedCount = data['verified'].sum()\n",
    "        reviewCount = data['asin'].count()\n",
    "        percentVerified = verifiedCount / reviewCount\n",
    "        new_row = {'asin': asin, 'numReviews': reviewCount, 'percentVerified': percentVerified, 'reviewText': ' '.join(data['reviewText']), 'summaryText': ' '.join(data['summary'])} \n",
    "        review_group_df = review_group_df.append(new_row, ignore_index = True)\n",
    "        \n",
    "    return (review_group_df, product_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddaf67f-9958-4427-852f-120974d5c5a9",
   "metadata": {},
   "source": [
    "PSEUDOCODE FOR WHAT WE NEED TO DO WITH THE TRAINING DATA:\n",
    " - Saahir/Amy starts by putting all of the reviews for a product in one string. This string is the DOCUMENT for the reviews of a product\n",
    " - I will then take the corpus and transform it to remove stopwords, punctuation, and lemmatize everything. This is the true \"bag of words\".\n",
    " - Then, I use the cleaned text data to train the Naive Bayes classifier\n",
    " - When we get test data, first clean in the same way, then use it in the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c8e5cb-dfb0-4602-b800-3f6cc25f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    file = open('en.txt')\n",
    "    stopwords = []\n",
    "    for line in file:\n",
    "        stopwords.append(line.rstrip())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a82e873-6a79-4d55-9c06-94041197a3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocess raw text datum involve task tokenize text i.e. split individual word remove stopword stem lemmatize word convert text numerical format input model split datum training testing set training set train model testing set evaluate performance'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_document(doc, remove_stopwords = True):\n",
    "    new_doc = \"\"\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(doc)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            new_doc += lemma + \" \"\n",
    "    return new_doc.rstrip()\n",
    "\n",
    "transform_document(\"First, you need to preprocess the raw text data. This may involve tasks like tokenizing the text (i.e., splitting it into individual words), removing stopwords, stemming or lemmatizing the words, and converting the text into a numerical format that can be used as input for the model. Then, you need to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944f9414-c74e-4287-8756-74e82d279fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(review_text, remove_stopwords = True):\n",
    "    word_bag = {}\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(review_text)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            if lemma in word_bag:\n",
    "                word_bag[lemma] += 1\n",
    "            else:\n",
    "                word_bag[lemma] = 1\n",
    "    return word_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c054d665-1ac0-4e05-9826-b96a2bc4bb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'this', 'third', 'one', 'the', 'and', 'document', 'second', 'be']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vocabulary_from_corpus(corpus, remove_stopwords = True):\n",
    "    vocab_set = set()\n",
    "    for document in corpus:\n",
    "        word_bag = bag_of_words(document, remove_stopwords)\n",
    "        for word in word_bag.keys():\n",
    "            vocab_set.add(word)\n",
    "    return list(vocab_set)\n",
    "\n",
    "def transform_document(doc, remove_stopwords = True):\n",
    "    new_doc = \"\"\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(doc)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            new_doc += lemma + \" \"\n",
    "    return new_doc.rstrip()\n",
    "\n",
    "vocabulary_from_corpus(['this is the first document', 'this document is the second document', 'and this is the third one', 'is this the first document'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a38ef6d-1722-4684-9d96-93b6e5790cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I am being handed a list of documents\", \"Each of these documents has several unique words\", \"The words will represent the class of each review\", \"I am also removing stopwords in order to make this make more sense\"]\n",
    "cleaned_corpus = [transform_document(doc) for doc in corpus]\n",
    "vocabulary = vocabulary_from_corpus(cleaned_corpus, True)\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), ('tfid', TfidfTransformer())]).fit(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c92fbe-6af3-432f-8a0a-fff1ea309c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "(review_group_df, product_df) = data_preprocessing(\"../devided_dataset_v2/CDs_and_Vinyl/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a846d49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Even tho I love this album, I am having problems playing it on my stereo system. This is the 4th CD that i have ordered as thinking there was something wrong with the CD\\'s itself. It plays perfectly on my portable CD player.......but it can\\'t play on my stereo system. I read that this recording was electronically enhanced for PC use. Don\\'t know what\\'s going on, as my stereo system is only 2 & 1/2 months old. It\\'s a Jenson system. Sigh.......maybe I\\'ll finally get lucky and find a *playable copy of this CD some day. Otherwise.....it\\'s an fantastic recording with beautiful songs by Ms. Nancy Wilson. Nancy Wilson is still one of the most distinctive and unique singers I have ever known! I love everything about her; Her voice, her classiness (and by the way, she \\'ALWAYS HAVE MAINTAINED HER SINCE OF CLASSINESS & STYLE SINCE I introduced myself to her music! I\\'m a avid jazz lover and she has \\'ALWAYS\\' been apart of mostly every album she has made! I purchased 2 CD\\'s at the time of my most recent purchases of a few CD\\'s I\\'ve had to replace, because they always seem to disappear somehow.  I purchased this one, \\'NANCY WILSON - GREATEST HITS\\', and, \\'IF I HAD MY WAY\\' and I love them both equally. So....if you are entertaining your friends who like jazz music, in love, just laid back to hear music that you never tire of...THEN \\'NANCY\\' IS A MUST IN YOUR COLLECTION! Having been a Nancy Wilson fan for over twenty years, I heartily recommend her latest offering.  It is the perfect blend of her unigue vocal interpretations and able support from the studio musicians.  All the cuts  on the release are superb with special recognition given to the title cut,  the duet (with James Ingram) \"Wish You Were Here\" and the  seductive \"Anything for Your Love.\"  The CD contains an unlisted  bonus selection taken from an earlier collaboration with Barry Manilow.  This added gem only enhances an already flawless release.  Keep up the good  work, Nancy.  Your fans will forever be the winners.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_group_df['reviewText'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
