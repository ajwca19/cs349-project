{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06a99b21-db3c-44bc-ba47-c72d4a4f235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f76acec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5b12a9-5bbc-4884-9edd-f0a75da71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(path, test=False):\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    #create appropriate file path\n",
    "    if test == False:\n",
    "        pfilename = path + \"/product_training.json\"\n",
    "        rfilename = path + \"/review_training.json\"\n",
    "    else:\n",
    "        pfilename = path + \"/product_test.json\"\n",
    "        rfilename = path + \"/review_test.json\"\n",
    "    \n",
    "    #extract files as pandas dataframes\n",
    "    product_df = pd.read_json(pfilename)\n",
    "    \n",
    "    review_df = pd.read_json(rfilename).drop_duplicates(subset=[\"reviewerID\", \"unixReviewTime\"], keep=\"first\")\n",
    "    ## 11.66 seconds to get to here\n",
    "    \n",
    "    review_df.drop(columns=[\"reviewerID\",\"vote\", \"unixReviewTime\",\"reviewTime\",\"style\",\"reviewerName\",\"image\"], axis=1 ,inplace=True)\n",
    "    \n",
    "    review_df['reviewText'].fillna(\"\", inplace=True)\n",
    "    review_df['summary'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    review_df.sort_values('asin', inplace = True)\n",
    "    product_df.sort_values('asin', inplace = True)\n",
    "    \n",
    "    group = review_df.groupby(\"asin\")\n",
    "    \n",
    "    #review_group_df = pd.DataFrame(columns = ['asin', 'numReviews', 'percentVerified','reviewText','summaryText', 'awesomeness'])\n",
    "    \n",
    "    # about the same amount of time to get to here\n",
    "    start_time = time.time()\n",
    "    datalist = []\n",
    "    count = 0\n",
    "    #awesome_pos = 0\n",
    "    for asin, data in group:\n",
    "        verifiedCount = data['verified'].sum()\n",
    "        reviewCount = data['asin'].count()\n",
    "        percentVerified = verifiedCount / reviewCount\n",
    "        reviewText = ' '.join(data['reviewText'])\n",
    "        #reviewText = ' '.join(transform_document(x) for x in data['reviewText'])\n",
    "        #summaryText = \"\"\n",
    "        summaryText = ' '.join(data['summary'])\n",
    "        #summaryText = ' '.join(transform_document(x) for x in data['summary'])\n",
    "        #reviewText = transform_document(' '.join(data['reviewText']))\n",
    "        #summaryText = transform_document(' '.join(data['summary']))\n",
    "        #awesomeness = 0\n",
    "        while (product_df['asin'][count] != asin):\n",
    "               count = count + 1\n",
    "        \n",
    "        awesomeness = product_df['awesomeness'][count]\n",
    "        #awesome_pos = awesome_pos + reviewCount\n",
    "        #awesomeness = product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0] #might be slow\n",
    "        datalist.append([asin,  reviewCount, percentVerified, reviewText, summaryText, awesomeness])\n",
    "        \n",
    "        count = count + 1\n",
    "        #if count > 100:\n",
    "        #    break\n",
    "        \n",
    "        '''new_row = {'asin': asin, \n",
    "                   'numReviews': reviewCount, \n",
    "                   'percentVerified': percentVerified, \n",
    "                   'reviewText': transform_document(' '.join(data['reviewText'])), \n",
    "                   'summaryText': transform_document(' '.join(data['summary'])), \n",
    "                   'awesomeness': product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0]} \n",
    "        review_group_df = review_group_df.append(new_row, ignore_index = True)\n",
    "         '''\n",
    "    review_group_df = pd.DataFrame(datalist,columns =['asin', 'numReviews', 'percentVerified','reviewText','summaryText', 'awesomeness'])    \n",
    "    \n",
    "    review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data.json')\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    \n",
    "    return review_group_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c92fbe-6af3-432f-8a0a-fff1ea309c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.382253885269165\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed Data Generated (Reviews and Summaries aggregated, no NLP processing)\n",
    "review_group_df = data_preprocessing(\"../devided_dataset_v2/CDs_and_Vinyl/train\")\n",
    "#review_group_df.head()\n",
    "#review_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "deb8624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline based on countvectorizer, tfidf, multinomial NB (can try support vector machine or decision tree also)\n",
    "# will want to use the k-fold cross-validation on this\n",
    "# will want to figure out how to combine the tfidf vectors for the review and summary with the other features we had in mind\n",
    "# e.g., the percent of verified reviews\n",
    "#text_clf = Pipeline([('vect', make_column_transformer((CountVectorizer(), ['reviewText']), (CountVectorizer(), ['summaryText'])) ), \n",
    "#                     ('tfidf', make_column_transformer((TfidfTransformer(), [0]), (TfidfTransformer(), ['summaryText'])) ), \n",
    "#                     ('clf', MultinomialNB())] )\n",
    "#text_clf.fit(review_group_df['reviewText'] ,review_group_df['awesomeness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f5b61d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m             \u001b[0mall_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2888/561530270.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_group_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview_group_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'awesomeness'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m_validate_remainder\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m             \u001b[0mcols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_column_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[0mall_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             raise ValueError(\"Specifying the columns using strings is only \"\n\u001b[0m\u001b[0;32m    377\u001b[0m                              \"supported for pandas DataFrames\")\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "text_clf.fit(review_group_df, review_group_df['awesomeness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a7c60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check - if the model can't predict the results for the training data, then it really didn't work\n",
    "predicted_reviews = text_clf.predict(review_group_df['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34464490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6630697622408901"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted_reviews == review_group_df['awesomeness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8edefa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Text Processing (Count Vectorizer and TFIDF Transform)\n",
    "countvect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1662f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewTrainCounts = countvect.fit_transform(review_group_df['reviewText'])\n",
    "summaryTrainCounts = countvect.fit_transform(review_group_df['summaryText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52c714bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71543, 55093)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reviewTrainCounts.shape\n",
    "summaryTrainCounts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02398d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71543, 55093)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewTransformer = TfidfTransformer(use_idf=True).fit(reviewTrainCounts)\n",
    "summaryTransformer = TfidfTransformer(use_idf=True).fit(summaryTrainCounts)\n",
    "reviewTrain_tfidf = reviewTransformer.transform(reviewTrainCounts)\n",
    "summaryTrain_tfidf = summaryTransformer.transform(summaryTrainCounts)\n",
    "\n",
    "summaryTrain_tfidf.shape\n",
    "#reviewTrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0e0c0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71543x311376 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25824726 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewTrain_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3d4ea58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "blocks must be 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2888/3014572690.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviewTrain_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummaryTrain_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview_group_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'awesomeness'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\construct.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     \"\"\"\n\u001b[1;32m--> 467\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\construct.py\u001b[0m in \u001b[0;36mbmat\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'blocks must be 2-D'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: blocks must be 2-D"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(hstack(reviewTrain_tfidf, summaryTrain_tfidf), review_group_df['awesomeness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddaf67f-9958-4427-852f-120974d5c5a9",
   "metadata": {},
   "source": [
    "PSEUDOCODE FOR WHAT WE NEED TO DO WITH THE TRAINING DATA:\n",
    " - Saahir/Amy starts by putting all of the reviews for a product in one string. This string is the DOCUMENT for the reviews of a product\n",
    " - I will then take the corpus and transform it to remove stopwords, punctuation, and lemmatize everything. This is the true \"bag of words\".\n",
    " - Then, I use the cleaned text data to train the Naive Bayes classifier\n",
    " - When we get test data, first clean in the same way, then use it in the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a38ef6d-1722-4684-9d96-93b6e5790cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I am being handed a list of documents\", \"Each of these documents has several unique words\", \"The words will represent the class of each review\", \"I am also removing stopwords in order to make this make more sense\"]\n",
    "cleaned_corpus = [transform_document(doc) for doc in corpus]\n",
    "vocabulary = vocabulary_from_corpus(cleaned_corpus, True)\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), \n",
    "                 ('tfid', TfidfTransformer())]).fit(cleaned_corpus)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df.loc[product_df['asin'] == '0000B049F5B33CD310EB1AB236E20191', 'awesomeness'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bb63e",
   "metadata": {},
   "source": [
    "NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c8e5cb-dfb0-4602-b800-3f6cc25f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    file = open('en.txt')\n",
    "    stopwords = []\n",
    "    for line in file:\n",
    "        stopwords.append(line.rstrip())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c4e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_corpus(review_group):\n",
    "    stopwords = get_stopwords()\n",
    "    reviewTextSet = review_group['reviewText']\n",
    "    for index in review_group.index:\n",
    "        curr_parsed = nlp(reviewTextSet[index].lower())\n",
    "        doclist = []\n",
    "        for token in curr_parsed:\n",
    "            lemma = token.lemma_\n",
    "            if not(re.match(\"[a-z0-9]+\", lemma)):\n",
    "                continue\n",
    "            if lemma not in stopwords:\n",
    "                doclist.append(lemma)\n",
    "        reviewTextSet[index] = \" \".join(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9a82e873-6a79-4d55-9c06-94041197a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_document(doc, remove_stopwords = True):\n",
    "    #new_doc = \"\"\n",
    "    stopwords = get_stopwords() # is this slow?\n",
    "    parsed_text = nlp(doc) # is this slow\n",
    "    doclist = []\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            doclist.append(lemma) # this is less slow?\n",
    "    return \" \".join(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "037bd739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.1 ms ± 1.53 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit transform_document(\"First, you need to preprocess the raw text data. This may involve tasks like tokenizing the text (i.e., splitting it into individual words), removing stopwords, stemming or lemmatizing the words, and converting the text into a numerical format that can be used as input for the model. Then, you need to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "944f9414-c74e-4287-8756-74e82d279fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(review_text, remove_stopwords = True):\n",
    "    word_bag = {}\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(review_text)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            if lemma in word_bag:\n",
    "                word_bag[lemma] += 1\n",
    "            else:\n",
    "                word_bag[lemma] = 1\n",
    "    return word_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c054d665-1ac0-4e05-9826-b96a2bc4bb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'the', 'second', 'third', 'be', 'and', 'this', 'one', 'document']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vocabulary_from_corpus(corpus, remove_stopwords = True):\n",
    "    vocab_set = set()\n",
    "    for document in corpus:\n",
    "        word_bag = bag_of_words(document, remove_stopwords)\n",
    "        for word in word_bag.keys():\n",
    "            vocab_set.add(word)\n",
    "    return list(vocab_set)\n",
    "vocabulary_from_corpus(['this is the first document', 'this document is the second document', 'and this is the third one', 'is this the first document'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da656ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/product_training.json')\n",
    "review_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/review_training.json')\n",
    "#len(product_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098d8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "teststring = \"First, you need to preprocess the raw text data. This may involve tasks like tokenizing the text (i.e., splitting it into individual words), removing stopwords, stemming or lemmatizing the words, and converting the text into a numerical format that can be used as input for the model. Then, you need to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance.\"\n",
    "#teststring.lower()\n",
    "parsed = nlp(teststring.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f9117d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   asin  numReviews  percentVerified  \\\n",
      "0      0000B049F5B33CD310EB1AB236E20191           3         0.666667   \n",
      "1      00018184A9EC4D270219A296B2580303          14         0.071429   \n",
      "2      000281A9CAC43FF1F335726A390636DA           1         0.000000   \n",
      "3      00030884DF109F325638A6BFD5B13CFF          20         0.550000   \n",
      "4      000325BA25966B5FC701D5D2B5DBA4E0           3         1.000000   \n",
      "...                                 ...         ...              ...   \n",
      "71538  FFFDD3C72D23AF858D6E0ED92612370D          41         0.341463   \n",
      "71539  FFFDDE284A73B29B320381487EC7DE9E           4         0.500000   \n",
      "71540  FFFEB3EE2372807964F024707D50FB21           2         1.000000   \n",
      "71541  FFFF4545AB232D81D0F9B208388BB7AA           5         0.600000   \n",
      "71542  FFFF5A3D9CB0B40FF0FE6B95F05D26FE          23         0.043478   \n",
      "\n",
      "                                              reviewText  \\\n",
      "0      Even tho I love this album, I am having proble...   \n",
      "1      I have been a fan of GU's releases since i can...   \n",
      "2      I made the mistake buying this album after lis...   \n",
      "3      Wow!  A must hear! Bob Marley at his best. Wha...   \n",
      "4      Soft, melodic notes dot moving waves of ethere...   \n",
      "...                                                  ...   \n",
      "71538  The bright yellow case on Kiss' sophomore albu...   \n",
      "71539  I picked up this CD for about $6 US and I have...   \n",
      "71540  Pop music has a short memory, but Kevin Roland...   \n",
      "71541  I ordered the album as soon as I stumbled it, ...   \n",
      "71542  Opinions are funny things. Everyone has differ...   \n",
      "\n",
      "                                             summaryText  \n",
      "0      I LOVE NANCY THE BEAUTIFUL LEGEND, NANCY WILSO...  \n",
      "1      One of GU's best The King of Progressive House...  \n",
      "2                                           Bad Business  \n",
      "3      Maybe the Greatest Live Reggae Album Ever Bob ...  \n",
      "4      Light Notes Robin Miller/Transcendence relaxin...  \n",
      "...                                                  ...  \n",
      "71538  'Cause baby's got the feeling, baby wants a sh...  \n",
      "71539  A leisurely stroll through the country AN HONE...  \n",
      "71540  A strong comeback by a troubled artist The 4th...  \n",
      "71541  EXCELLENT concept album, but shouldn't be the ...  \n",
      "71542  MAKE YOUR OWN DECISION few good songs Worse th...  \n",
      "\n",
      "[71543 rows x 5 columns]\n",
      "       awesomeness\n",
      "0                1\n",
      "1                0\n",
      "2                0\n",
      "3                1\n",
      "4                0\n",
      "...            ...\n",
      "71538            1\n",
      "71539            1\n",
      "71540            0\n",
      "71541            1\n",
      "71542            1\n",
      "\n",
      "[71543 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "string_features = ['reviewText', 'summaryText']\n",
    "string_transformer = Pipeline(\n",
    "    [('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer())]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"str\", string_transformer, string_features)\n",
    "    ]\n",
    ")\n",
    "x = review_group_df.loc[:, review_group_df.columns != 'awesomeness']\n",
    "y = review_group_df.loc[:, review_group_df.columns == 'awesomeness']\n",
    "\n",
    "\n",
    "kf = KFold(splits = 10, shuffle = True)\n",
    "for i, (train_index, test_index) in enumerate(kf.split()):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffd023-19c0-4f05-8688-208683168941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
