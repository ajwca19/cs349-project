{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06a99b21-db3c-44bc-ba47-c72d4a4f235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca5b12a9-5bbc-4884-9edd-f0a75da71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(path, test=False):\n",
    "    #create appropriate file path\n",
    "    if test == False:\n",
    "        pfilename = path + \"/product_training.json\"\n",
    "        rfilename = path + \"/review_training.json\"\n",
    "    else:\n",
    "        pfilename = path + \"/product_test.json\"\n",
    "        rfilename = path + \"/review_test.json\"\n",
    "    \n",
    "    #extract files as pandas dataframes\n",
    "    product_df = pd.read_json(pfilename)\n",
    "    review_df = pd.read_json(rfilename).drop_duplicates(subset=[\"reviewerID\", \"unixReviewTime\"], keep=\"first\")\n",
    "    grouped_reviews = review_df.groupby('asin')\n",
    "    print(grouped_reviews)\n",
    "    asin_review_data_dict = {}\n",
    "    #for ind in product_df.index:\n",
    "        #print(ind)\n",
    "        #review_group = grouped_reviews.get_group(review_df['asin'][ind])\n",
    "        #asin_review_data_dict.update({product_df['asin'][ind] : review_group})\n",
    "        \n",
    "    return (asin_review_data_dict, product_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddaf67f-9958-4427-852f-120974d5c5a9",
   "metadata": {},
   "source": [
    "PSEUDOCODE FOR WHAT WE NEED TO DO WITH THE TRAINING DATA:\n",
    " - Saahir/Amy starts by putting all of the reviews for a product in one string. This string is the DOCUMENT for the reviews of a product\n",
    " - I will then take the corpus and transform it to remove stopwords, punctuation, and lemmatize everything. This is the true \"bag of words\".\n",
    " - Then, I use the cleaned text data to train the Naive Bayes classifier\n",
    " - When we get test data, first clean in the same way, then use it in the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c8e5cb-dfb0-4602-b800-3f6cc25f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    file = open('en.txt')\n",
    "    stopwords = []\n",
    "    for line in file:\n",
    "        stopwords.append(line.rstrip())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a82e873-6a79-4d55-9c06-94041197a3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocess raw text datum involve task tokenize text i.e. split individual word remove stopword stem lemmatize word convert text numerical format input model split datum training testing set training set train model testing set evaluate performance'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_document(doc, remove_stopwords = True):\n",
    "    new_doc = \"\"\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(doc)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            new_doc += lemma + \" \"\n",
    "    return new_doc.rstrip()\n",
    "\n",
    "transform_document(\"First, you need to preprocess the raw text data. This may involve tasks like tokenizing the text (i.e., splitting it into individual words), removing stopwords, stemming or lemmatizing the words, and converting the text into a numerical format that can be used as input for the model. Then, you need to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "944f9414-c74e-4287-8756-74e82d279fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(review_text, remove_stopwords = True):\n",
    "    word_bag = {}\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(review_text)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            if lemma in word_bag:\n",
    "                word_bag[lemma] += 1\n",
    "            else:\n",
    "                word_bag[lemma] = 1\n",
    "    return word_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c054d665-1ac0-4e05-9826-b96a2bc4bb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hand list document document unique word word represent class review'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vocabulary_from_corpus(corpus, remove_stopwords = True):\n",
    "    vocab_set = set()\n",
    "    for document in corpus:\n",
    "        word_bag = bag_of_words(document, remove_stopwords)\n",
    "        for word in word_bag.keys():\n",
    "            vocab_set.add(word)\n",
    "    return list(vocab_set)\n",
    "\n",
    "def transform_document(doc, remove_stopwords = True):\n",
    "    new_doc = \"\"\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(doc)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            new_doc += lemma + \" \"\n",
    "    return new_doc.rstrip()\n",
    "\n",
    "vocabulary_from_corpus(['this is the first document', 'this document is the second document', 'and this is the third one', 'is this the first document'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a38ef6d-1722-4684-9d96-93b6e5790cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hand list document', 'document unique word', 'word represent class review', 'remove stopword order make make sense']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.91629073, 1.91629073, 1.91629073, 1.51082562, 1.91629073,\n",
       "       1.91629073, 1.51082562, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"I am being handed a list of documents\", \"Each of these documents has several unique words\", \"The words will represent the class of each review\", \"I am also removing stopwords in order to make this make more sense\"]\n",
    "cleaned_corpus = [transform_document(doc) for doc in corpus]\n",
    "vocabulary = vocabulary_from_corpus(cleaned_corpus, True)\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), ('tfid', TfidfTransformer())]).fit(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2cfadc0-aa2e-4455-a879-eb6240951a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>awesomeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000B049F5B33CD310EB1AB236E20191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018184A9EC4D270219A296B2580303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000281A9CAC43FF1F335726A390636DA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00030884DF109F325638A6BFD5B13CFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000325BA25966B5FC701D5D2B5DBA4E0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73077</th>\n",
       "      <td>FFFDD3C72D23AF858D6E0ED92612370D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73078</th>\n",
       "      <td>FFFDDE284A73B29B320381487EC7DE9E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73079</th>\n",
       "      <td>FFFEB3EE2372807964F024707D50FB21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73080</th>\n",
       "      <td>FFFF4545AB232D81D0F9B208388BB7AA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73081</th>\n",
       "      <td>FFFF5A3D9CB0B40FF0FE6B95F05D26FE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73082 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   asin  awesomeness\n",
       "0      0000B049F5B33CD310EB1AB236E20191            1\n",
       "1      00018184A9EC4D270219A296B2580303            0\n",
       "2      000281A9CAC43FF1F335726A390636DA            0\n",
       "3      00030884DF109F325638A6BFD5B13CFF            1\n",
       "4      000325BA25966B5FC701D5D2B5DBA4E0            0\n",
       "...                                 ...          ...\n",
       "73077  FFFDD3C72D23AF858D6E0ED92612370D            1\n",
       "73078  FFFDDE284A73B29B320381487EC7DE9E            1\n",
       "73079  FFFEB3EE2372807964F024707D50FB21            0\n",
       "73080  FFFF4545AB232D81D0F9B208388BB7AA            1\n",
       "73081  FFFF5A3D9CB0B40FF0FE6B95F05D26FE            1\n",
       "\n",
       "[73082 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "86c92fbe-6af3-432f-8a0a-fff1ea309c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fbfe0995460>\n"
     ]
    }
   ],
   "source": [
    "(review_group_dict, product_df) = data_preprocessing(\"../devided_dataset_v2/CDs_and_Vinyl/train\")\n",
    "#review_group_dict\n",
    "#review_group_dict['0000B049F5B33CD310EB1AB236E20191']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e4f44-af61-48e2-b580-263b92383075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
