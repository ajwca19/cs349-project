{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a99b21-db3c-44bc-ba47-c72d4a4f235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76acec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/adenweiser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca5b12a9-5bbc-4884-9edd-f0a75da71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(path, test=False):\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    #create appropriate file path\n",
    "    if test == False:\n",
    "        pfilename = path + \"/product_training.json\"\n",
    "        rfilename = path + \"/review_training.json\"\n",
    "    else:\n",
    "        pfilename = path + \"/product_test.json\"\n",
    "        rfilename = path + \"/review_test.json\"\n",
    "    \n",
    "    #extract files as pandas dataframes\n",
    "    product_df = pd.read_json(pfilename)\n",
    "    \n",
    "    review_df = pd.read_json(rfilename).drop_duplicates(subset=[\"reviewerID\", \"unixReviewTime\"], keep=\"first\")\n",
    "    ## 11.66 seconds to get to here\n",
    "    \n",
    "    review_df.drop(columns=[\"reviewerID\",\"vote\", \"unixReviewTime\",\"reviewTime\",\"style\",\"reviewerName\",\"image\"], axis=1 ,inplace=True)\n",
    "    \n",
    "    review_df['reviewText'].fillna(\"\", inplace=True)\n",
    "    review_df['summary'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    review_df.sort_values('asin', inplace = True)\n",
    "    product_df.sort_values('asin', inplace = True)\n",
    "    \n",
    "    group = review_df.groupby(\"asin\")\n",
    "    \n",
    "    #review_group_df = pd.DataFrame(columns = ['asin', 'numReviews', 'percentVerified','reviewText','summaryText', 'awesomeness'])\n",
    "    \n",
    "    # about the same amount of time to get to here\n",
    "    start_time = time.time()\n",
    "    datalist = []\n",
    "    count = 0\n",
    "    #awesome_pos = 0\n",
    "    for asin, data in group:\n",
    "        verifiedCount = data['verified'].sum()\n",
    "        reviewCount = data['asin'].count()\n",
    "        percentVerified = verifiedCount / reviewCount\n",
    "        if count == 0:\n",
    "            print(type(data['reviewText']))\n",
    "        reviewText = ' '.join(data['reviewText'])\n",
    "        #reviewText = ' '.join(transform_document(x) for x in data['reviewText'])\n",
    "        #summaryText = \"\"\n",
    "        summaryText = ' '.join(data['summary'])\n",
    "        #summaryText = ' '.join(transform_document(x) for x in data['summary'])\n",
    "        #reviewText = transform_document(' '.join(data['reviewText']))\n",
    "        #summaryText = transform_document(' '.join(data['summary']))\n",
    "        #awesomeness = 0\n",
    "        \n",
    "        #SENTIMENT ANALYSIS CHUNK\n",
    "        (rev_means, rev_stdevs) = sentiment_analysis(data['reviewText'])\n",
    "        (sum_means, sum_stdevs) = sentiment_analysis(data['summary'])\n",
    "        while (product_df['asin'][count] != asin):\n",
    "               count = count + 1\n",
    "        \n",
    "        awesomeness = product_df['awesomeness'][count]\n",
    "        #awesome_pos = awesome_pos + reviewCount\n",
    "        #awesomeness = product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0] #might be slow\n",
    "        datalist.append([asin,  reviewCount, percentVerified, reviewText, \\\n",
    "                         summaryText, rev_means[0], rev_means[1], rev_means[2], \\\n",
    "                         rev_stdevs[0], rev_stdevs[1], rev_stdevs[2], sum_means[0], \\\n",
    "                         sum_means[1], sum_means[2], sum_stdevs[0], sum_stdevs[1], \\\n",
    "                         sum_stdevs[2], awesomeness])\n",
    "        \n",
    "        count = count + 1\n",
    "        #if count > 100:\n",
    "        #    break\n",
    "        \n",
    "        '''new_row = {'asin': asin, \n",
    "                   'numReviews': reviewCount, \n",
    "                   'percentVerified': percentVerified, \n",
    "                   'reviewText': transform_document(' '.join(data['reviewText'])), \n",
    "                   'summaryText': transform_document(' '.join(data['summary'])), \n",
    "                   'awesomeness': product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0]} \n",
    "        review_group_df = review_group_df.append(new_row, ignore_index = True)\n",
    "         '''\n",
    "    review_group_df = pd.DataFrame(datalist,columns =['asin', 'numReviews', 'percentVerified','reviewText','summaryText', \\\n",
    "                                                      'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                                      'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                                      'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev', \\\n",
    "                                                      'awesomeness'])    \n",
    "    \n",
    "    review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data_new.json')\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    \n",
    "    return review_group_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c92fbe-6af3-432f-8a0a-fff1ea309c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "1695.547775030136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>numReviews</th>\n",
       "      <th>percentVerified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summaryText</th>\n",
       "      <th>reviewPosMean</th>\n",
       "      <th>reviewNeuMean</th>\n",
       "      <th>reviewNegMean</th>\n",
       "      <th>reviewPosStDev</th>\n",
       "      <th>reviewNeuStDev</th>\n",
       "      <th>reviewNegStDev</th>\n",
       "      <th>summaryPosMean</th>\n",
       "      <th>summaryNeuMean</th>\n",
       "      <th>summaryNegMean</th>\n",
       "      <th>summaryPosStDev</th>\n",
       "      <th>summaryNeuStDev</th>\n",
       "      <th>summaryNegStDev</th>\n",
       "      <th>awesomeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000B049F5B33CD310EB1AB236E20191</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Even tho I love this album, I am having proble...</td>\n",
       "      <td>I LOVE NANCY THE BEAUTIFUL LEGEND, NANCY WILSO...</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.738333</td>\n",
       "      <td>0.037333</td>\n",
       "      <td>0.040262</td>\n",
       "      <td>0.038083</td>\n",
       "      <td>0.027154</td>\n",
       "      <td>0.471333</td>\n",
       "      <td>0.528667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420497</td>\n",
       "      <td>0.420497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018184A9EC4D270219A296B2580303</td>\n",
       "      <td>14</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>I have been a fan of GU's releases since i can...</td>\n",
       "      <td>One of GU's best The King of Progressive House...</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.805214</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.072115</td>\n",
       "      <td>0.077152</td>\n",
       "      <td>0.033816</td>\n",
       "      <td>0.192786</td>\n",
       "      <td>0.783786</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.217996</td>\n",
       "      <td>0.268650</td>\n",
       "      <td>0.087929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000281A9CAC43FF1F335726A390636DA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I made the mistake buying this album after lis...</td>\n",
       "      <td>Bad Business</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00030884DF109F325638A6BFD5B13CFF</td>\n",
       "      <td>20</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>Wow!  A must hear! Bob Marley at his best. Wha...</td>\n",
       "      <td>Maybe the Greatest Live Reggae Album Ever Bob ...</td>\n",
       "      <td>0.304300</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.021650</td>\n",
       "      <td>0.249407</td>\n",
       "      <td>0.237702</td>\n",
       "      <td>0.043391</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.717150</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.330916</td>\n",
       "      <td>0.325690</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000325BA25966B5FC701D5D2B5DBA4E0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Soft, melodic notes dot moving waves of ethere...</td>\n",
       "      <td>Light Notes Robin Miller/Transcendence relaxin...</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.052943</td>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>0.079333</td>\n",
       "      <td>0.780333</td>\n",
       "      <td>0.140333</td>\n",
       "      <td>0.137409</td>\n",
       "      <td>0.380474</td>\n",
       "      <td>0.243064</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71538</th>\n",
       "      <td>FFFDD3C72D23AF858D6E0ED92612370D</td>\n",
       "      <td>41</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>The bright yellow case on Kiss' sophomore albu...</td>\n",
       "      <td>'Cause baby's got the feeling, baby wants a sh...</td>\n",
       "      <td>0.280927</td>\n",
       "      <td>0.652244</td>\n",
       "      <td>0.066780</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>0.177106</td>\n",
       "      <td>0.044662</td>\n",
       "      <td>0.308610</td>\n",
       "      <td>0.618341</td>\n",
       "      <td>0.073049</td>\n",
       "      <td>0.307788</td>\n",
       "      <td>0.304994</td>\n",
       "      <td>0.155345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71539</th>\n",
       "      <td>FFFDDE284A73B29B320381487EC7DE9E</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>I picked up this CD for about $6 US and I have...</td>\n",
       "      <td>A leisurely stroll through the country AN HONE...</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.651750</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.311821</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.023580</td>\n",
       "      <td>0.305750</td>\n",
       "      <td>0.522250</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.353175</td>\n",
       "      <td>0.320673</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71540</th>\n",
       "      <td>FFFEB3EE2372807964F024707D50FB21</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Pop music has a short memory, but Kevin Roland...</td>\n",
       "      <td>A strong comeback by a troubled artist The 4th...</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.741500</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.127279</td>\n",
       "      <td>0.115258</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>0.251023</td>\n",
       "      <td>0.478711</td>\n",
       "      <td>0.228395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71541</th>\n",
       "      <td>FFFF4545AB232D81D0F9B208388BB7AA</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>I ordered the album as soon as I stumbled it, ...</td>\n",
       "      <td>EXCELLENT concept album, but shouldn't be the ...</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.775800</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.065238</td>\n",
       "      <td>0.085083</td>\n",
       "      <td>0.043895</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.667600</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.068813</td>\n",
       "      <td>0.140538</td>\n",
       "      <td>0.130139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71542</th>\n",
       "      <td>FFFF5A3D9CB0B40FF0FE6B95F05D26FE</td>\n",
       "      <td>23</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>Opinions are funny things. Everyone has differ...</td>\n",
       "      <td>MAKE YOUR OWN DECISION few good songs Worse th...</td>\n",
       "      <td>0.145739</td>\n",
       "      <td>0.739304</td>\n",
       "      <td>0.114870</td>\n",
       "      <td>0.083267</td>\n",
       "      <td>0.095374</td>\n",
       "      <td>0.088431</td>\n",
       "      <td>0.235565</td>\n",
       "      <td>0.571391</td>\n",
       "      <td>0.193043</td>\n",
       "      <td>0.342699</td>\n",
       "      <td>0.385184</td>\n",
       "      <td>0.337207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71543 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   asin  numReviews  percentVerified  \\\n",
       "0      0000B049F5B33CD310EB1AB236E20191           3         0.666667   \n",
       "1      00018184A9EC4D270219A296B2580303          14         0.071429   \n",
       "2      000281A9CAC43FF1F335726A390636DA           1         0.000000   \n",
       "3      00030884DF109F325638A6BFD5B13CFF          20         0.550000   \n",
       "4      000325BA25966B5FC701D5D2B5DBA4E0           3         1.000000   \n",
       "...                                 ...         ...              ...   \n",
       "71538  FFFDD3C72D23AF858D6E0ED92612370D          41         0.341463   \n",
       "71539  FFFDDE284A73B29B320381487EC7DE9E           4         0.500000   \n",
       "71540  FFFEB3EE2372807964F024707D50FB21           2         1.000000   \n",
       "71541  FFFF4545AB232D81D0F9B208388BB7AA           5         0.600000   \n",
       "71542  FFFF5A3D9CB0B40FF0FE6B95F05D26FE          23         0.043478   \n",
       "\n",
       "                                              reviewText  \\\n",
       "0      Even tho I love this album, I am having proble...   \n",
       "1      I have been a fan of GU's releases since i can...   \n",
       "2      I made the mistake buying this album after lis...   \n",
       "3      Wow!  A must hear! Bob Marley at his best. Wha...   \n",
       "4      Soft, melodic notes dot moving waves of ethere...   \n",
       "...                                                  ...   \n",
       "71538  The bright yellow case on Kiss' sophomore albu...   \n",
       "71539  I picked up this CD for about $6 US and I have...   \n",
       "71540  Pop music has a short memory, but Kevin Roland...   \n",
       "71541  I ordered the album as soon as I stumbled it, ...   \n",
       "71542  Opinions are funny things. Everyone has differ...   \n",
       "\n",
       "                                             summaryText  reviewPosMean  \\\n",
       "0      I LOVE NANCY THE BEAUTIFUL LEGEND, NANCY WILSO...       0.224000   \n",
       "1      One of GU's best The King of Progressive House...       0.141500   \n",
       "2                                           Bad Business       0.082000   \n",
       "3      Maybe the Greatest Live Reggae Album Ever Bob ...       0.304300   \n",
       "4      Light Notes Robin Miller/Transcendence relaxin...       0.266000   \n",
       "...                                                  ...            ...   \n",
       "71538  'Cause baby's got the feeling, baby wants a sh...       0.280927   \n",
       "71539  A leisurely stroll through the country AN HONE...       0.331250   \n",
       "71540  A strong comeback by a troubled artist The 4th...       0.234000   \n",
       "71541  EXCELLENT concept album, but shouldn't be the ...       0.164000   \n",
       "71542  MAKE YOUR OWN DECISION few good songs Worse th...       0.145739   \n",
       "\n",
       "       reviewNeuMean  reviewNegMean  reviewPosStDev  reviewNeuStDev  \\\n",
       "0           0.738333       0.037333        0.040262        0.038083   \n",
       "1           0.805214       0.053500        0.072115        0.077152   \n",
       "2           0.856000       0.062000        0.000000        0.000000   \n",
       "3           0.674100       0.021650        0.249407        0.237702   \n",
       "4           0.726667       0.007333        0.052943        0.047078   \n",
       "...              ...            ...             ...             ...   \n",
       "71538       0.652244       0.066780        0.197260        0.177106   \n",
       "71539       0.651750       0.017000        0.311821        0.305770   \n",
       "71540       0.741500       0.024500        0.127279        0.115258   \n",
       "71541       0.775800       0.060400        0.065238        0.085083   \n",
       "71542       0.739304       0.114870        0.083267        0.095374   \n",
       "\n",
       "       reviewNegStDev  summaryPosMean  summaryNeuMean  summaryNegMean  \\\n",
       "0            0.027154        0.471333        0.528667        0.000000   \n",
       "1            0.033816        0.192786        0.783786        0.023500   \n",
       "2            0.000000        0.000000        0.222000        0.778000   \n",
       "3            0.043391        0.274500        0.717150        0.008350   \n",
       "4            0.012702        0.079333        0.780333        0.140333   \n",
       "...               ...             ...             ...             ...   \n",
       "71538        0.044662        0.308610        0.618341        0.073049   \n",
       "71539        0.023580        0.305750        0.522250        0.172000   \n",
       "71540        0.012021        0.177500        0.661500        0.161500   \n",
       "71541        0.043895        0.274200        0.667600        0.058200   \n",
       "71542        0.088431        0.235565        0.571391        0.193043   \n",
       "\n",
       "       summaryPosStDev  summaryNeuStDev  summaryNegStDev  awesomeness  \n",
       "0             0.420497         0.420497         0.000000            1  \n",
       "1             0.217996         0.268650         0.087929            0  \n",
       "2             0.000000         0.000000         0.000000            0  \n",
       "3             0.330916         0.325690         0.037342            1  \n",
       "4             0.137409         0.380474         0.243064            0  \n",
       "...                ...              ...              ...          ...  \n",
       "71538         0.307788         0.304994         0.155345            1  \n",
       "71539         0.353175         0.320673         0.344000            1  \n",
       "71540         0.251023         0.478711         0.228395            0  \n",
       "71541         0.068813         0.140538         0.130139            1  \n",
       "71542         0.342699         0.385184         0.337207            1  \n",
       "\n",
       "[71543 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessed Data Generated (Reviews and Summaries aggregated, no NLP processing)\n",
    "review_group_df = data_preprocessing(\"../devided_dataset_v2/CDs_and_Vinyl/train\")\n",
    "#review_group_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data.json')\n",
    "review_group_df\n",
    "#review_group_df.to_json(\"preprocessed.json\")\n",
    "#review_group_df.head()\n",
    "#review_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663aa55a-fe39-4c15-a6cb-27f4fe073e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.13540000000000002, 0.3574, 0.5072),\n",
       " (0.30276360415347153, 0.12722539054764187, 0.31017366103523364))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment_analysis(docs):\n",
    "    sentiments = []\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    for doc in docs:\n",
    "        polarities = sid.polarity_scores(doc)\n",
    "        sentiments.append((polarities['pos'], polarities['neu'], polarities['neg']))\n",
    "    if len(sentiments) == 1:\n",
    "        return (sentiments[0], (0, 0, 0))\n",
    "    else:\n",
    "        pos = [sents[0] for sents in sentiments]\n",
    "        neu = [sents[1] for sents in sentiments]\n",
    "        neg = [sents[2] for sents in sentiments]\n",
    "        return ((statistics.mean(pos), statistics.mean(neu), statistics.mean(neg)), (statistics.stdev(pos), statistics.stdev(neu), statistics.stdev(neg)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "473a32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_transformer = Pipeline(\n",
    "    steps = [('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer())]\n",
    ")\n",
    "wordbagger = ColumnTransformer(\n",
    "    transformers=[(\"rev\", string_transformer, 'reviewText'), \n",
    "                  (\"sum\", string_transformer, 'summaryText')]\n",
    "    , remainder='passthrough'\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps = [(\"wordbag\", wordbagger), \n",
    "                        (\"scale\", MaxAbsScaler()), \n",
    "                        ('classifier', LogisticRegression(solver=\"newton-cg\", C=0.05, max_iter = 500, n_jobs=-1))])\n",
    "#x = review_group_df.filter(['numReviews', 'percentVerified', 'reviewText', 'summaryText'])\n",
    "#clf = LogisticRegression(tol = 0.001, max_iter = 150)\n",
    "#clf.fit(x,y)\n",
    "review_features = review_group_df.filter(['numReviews', 'percentVerified', 'reviewText', 'summaryText', \\\n",
    "                                          'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                          'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                          'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev'])\n",
    "y = review_group_df.filter(['awesomeness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7a6713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I started at 1683412352.128564\n",
      "5.099081182479859\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"I started at \" + str(start))\n",
    "# this runs the k-fold cross-validation automatically?\n",
    "cv10_results = cross_validate(clf, review_features, np.ravel(y), cv=10, n_jobs = -1, scoring = ['f1_macro', 'precision', 'recall'])\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f2d2011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([199.44159508, 196.63241124, 199.27014112, 202.24006891,\n",
      "       204.62397313, 203.71922731, 206.31563592, 203.55576992,\n",
      "        80.6432488 ,  78.14303589]), 'score_time': array([11.46691585, 11.90396762, 10.87090492, 10.41480803,  9.86779881,\n",
      "        9.37574887,  6.67444396,  7.19067693,  4.62899399,  4.78877211]), 'test_f1_macro': array([0.63173963, 0.62821132, 0.61948286, 0.62713879, 0.63064021,\n",
      "       0.6360395 , 0.62920227, 0.63288555, 0.63360341, 0.63436965]), 'test_precision': array([0.641368  , 0.63919717, 0.63161609, 0.63857988, 0.6397862 ,\n",
      "       0.64593188, 0.63994341, 0.64462018, 0.64262372, 0.64527672]), 'test_recall': array([0.72071598, 0.71255594, 0.70676494, 0.71018689, 0.72466439,\n",
      "       0.71887339, 0.71439853, 0.70808107, 0.72466439, 0.71221696])}\n",
      "0.6303313201814787\n",
      "0.004735812711854968\n",
      "0.6408943239310696\n",
      "0.004191715579223048\n",
      "0.7153122457740535\n",
      "0.006552655106527759\n"
     ]
    }
   ],
   "source": [
    "print(cv10_results)\n",
    "print(statistics.mean(cv10_results['test_f1_macro']))\n",
    "print(statistics.stdev(cv10_results['test_f1_macro']))\n",
    "print(statistics.mean(cv10_results['test_precision']))\n",
    "print(statistics.stdev(cv10_results['test_precision']))\n",
    "print(statistics.mean(cv10_results['test_recall']))\n",
    "print(statistics.stdev(cv10_results['test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d13e321a-2bf9-47f3-a4a6-c9dd1e4aec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm starting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.023726880550385\n"
     ]
    }
   ],
   "source": [
    "solvers = ['newton-cg']\n",
    "jobs = [-1]\n",
    "maxiter = [100, 500, 1000]\n",
    "c = [5, 2, 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01]\n",
    "param_grid={'classifier__C':c, 'classifier__solver':solvers, 'classifier__n_jobs':jobs, 'classifier__max_iter':maxiter}\n",
    "grid_clf = GridSearchCV(clf, param_grid, n_jobs=-1)\n",
    "start = time.time()\n",
    "print(\"I'm starting!\")\n",
    "grid_clf.fit(review_features, np.ravel(y))\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cea6b793-832b-4a98-bb2b-95193533a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "562fb612-31e0-43a7-9a47-cca8d896d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__max_iter</th>\n",
       "      <th>param_classifier__n_jobs</th>\n",
       "      <th>param_classifier__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.902761</td>\n",
       "      <td>3.981755</td>\n",
       "      <td>22.498697</td>\n",
       "      <td>0.715377</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.581173</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.586735</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263.848393</td>\n",
       "      <td>8.369911</td>\n",
       "      <td>22.481720</td>\n",
       "      <td>1.092433</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__max_iter': 5...</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.581173</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.586735</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258.521113</td>\n",
       "      <td>0.950623</td>\n",
       "      <td>21.237789</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.581173</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.586735</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>242.788301</td>\n",
       "      <td>6.899337</td>\n",
       "      <td>26.501668</td>\n",
       "      <td>2.905537</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.587253</td>\n",
       "      <td>0.587882</td>\n",
       "      <td>0.597666</td>\n",
       "      <td>0.594353</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.592958</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>245.064487</td>\n",
       "      <td>12.992323</td>\n",
       "      <td>22.550827</td>\n",
       "      <td>1.717254</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__max_iter': 5...</td>\n",
       "      <td>0.587253</td>\n",
       "      <td>0.587882</td>\n",
       "      <td>0.597666</td>\n",
       "      <td>0.594353</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.592958</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>230.603351</td>\n",
       "      <td>2.580260</td>\n",
       "      <td>22.640238</td>\n",
       "      <td>0.726609</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.587253</td>\n",
       "      <td>0.587882</td>\n",
       "      <td>0.597666</td>\n",
       "      <td>0.594353</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.592958</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>217.788828</td>\n",
       "      <td>4.732246</td>\n",
       "      <td>23.282278</td>\n",
       "      <td>1.899143</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.596478</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.604697</td>\n",
       "      <td>0.600241</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>219.208971</td>\n",
       "      <td>2.917052</td>\n",
       "      <td>21.258987</td>\n",
       "      <td>0.611024</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__max_iter': 5...</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.596478</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.604697</td>\n",
       "      <td>0.600241</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>214.697790</td>\n",
       "      <td>8.029622</td>\n",
       "      <td>23.524169</td>\n",
       "      <td>2.573233</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.596478</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.604697</td>\n",
       "      <td>0.600241</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202.574223</td>\n",
       "      <td>15.012335</td>\n",
       "      <td>22.778429</td>\n",
       "      <td>3.025260</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__max_iter':...</td>\n",
       "      <td>0.603816</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.613573</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>203.391861</td>\n",
       "      <td>2.496271</td>\n",
       "      <td>21.045616</td>\n",
       "      <td>0.183373</td>\n",
       "      <td>0.5</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__max_iter':...</td>\n",
       "      <td>0.603816</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.613573</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>193.466252</td>\n",
       "      <td>13.984599</td>\n",
       "      <td>22.463085</td>\n",
       "      <td>2.696157</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__max_iter':...</td>\n",
       "      <td>0.603816</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.613573</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>182.137830</td>\n",
       "      <td>19.212568</td>\n",
       "      <td>21.655008</td>\n",
       "      <td>1.693563</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__max_iter':...</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.626669</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>183.678528</td>\n",
       "      <td>2.561745</td>\n",
       "      <td>21.394557</td>\n",
       "      <td>0.871452</td>\n",
       "      <td>0.2</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__max_iter':...</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.626669</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>175.311657</td>\n",
       "      <td>12.944425</td>\n",
       "      <td>21.336044</td>\n",
       "      <td>1.767870</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__max_iter':...</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.626669</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>178.296248</td>\n",
       "      <td>2.926505</td>\n",
       "      <td>21.630373</td>\n",
       "      <td>0.291683</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__max_iter':...</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.625690</td>\n",
       "      <td>0.636732</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.631662</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>172.294253</td>\n",
       "      <td>10.734698</td>\n",
       "      <td>22.667302</td>\n",
       "      <td>2.671143</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__max_iter':...</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.625690</td>\n",
       "      <td>0.636732</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.631662</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>173.066506</td>\n",
       "      <td>7.797931</td>\n",
       "      <td>22.042321</td>\n",
       "      <td>2.201104</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__max_iter':...</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.625690</td>\n",
       "      <td>0.636732</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.631662</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>160.185623</td>\n",
       "      <td>6.154684</td>\n",
       "      <td>22.120637</td>\n",
       "      <td>1.715451</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.05, 'classifier__max_iter'...</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>0.638454</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.635814</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>154.716878</td>\n",
       "      <td>4.048059</td>\n",
       "      <td>23.099909</td>\n",
       "      <td>2.722425</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.05, 'classifier__max_iter'...</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>0.638454</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.635814</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>160.146975</td>\n",
       "      <td>4.332940</td>\n",
       "      <td>21.622918</td>\n",
       "      <td>0.544349</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.05, 'classifier__max_iter'...</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>0.638454</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.635814</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>141.816327</td>\n",
       "      <td>3.315259</td>\n",
       "      <td>23.385035</td>\n",
       "      <td>1.704255</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.02, 'classifier__max_iter'...</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>140.778459</td>\n",
       "      <td>8.244477</td>\n",
       "      <td>23.098364</td>\n",
       "      <td>3.046290</td>\n",
       "      <td>0.02</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.02, 'classifier__max_iter'...</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>144.068076</td>\n",
       "      <td>2.543079</td>\n",
       "      <td>21.405944</td>\n",
       "      <td>0.622739</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.02, 'classifier__max_iter'...</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>140.789716</td>\n",
       "      <td>5.046037</td>\n",
       "      <td>23.505679</td>\n",
       "      <td>1.898376</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__max_iter'...</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>0.622405</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.632793</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>141.560911</td>\n",
       "      <td>12.842808</td>\n",
       "      <td>20.430560</td>\n",
       "      <td>1.227738</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__max_iter'...</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>0.622405</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.632793</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>109.830957</td>\n",
       "      <td>4.216425</td>\n",
       "      <td>12.254573</td>\n",
       "      <td>3.053849</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__max_iter'...</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>0.622405</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.632793</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      265.902761      3.981755        22.498697        0.715377   \n",
       "1      263.848393      8.369911        22.481720        1.092433   \n",
       "2      258.521113      0.950623        21.237789        0.196200   \n",
       "3      242.788301      6.899337        26.501668        2.905537   \n",
       "4      245.064487     12.992323        22.550827        1.717254   \n",
       "5      230.603351      2.580260        22.640238        0.726609   \n",
       "6      217.788828      4.732246        23.282278        1.899143   \n",
       "7      219.208971      2.917052        21.258987        0.611024   \n",
       "8      214.697790      8.029622        23.524169        2.573233   \n",
       "9      202.574223     15.012335        22.778429        3.025260   \n",
       "10     203.391861      2.496271        21.045616        0.183373   \n",
       "11     193.466252     13.984599        22.463085        2.696157   \n",
       "12     182.137830     19.212568        21.655008        1.693563   \n",
       "13     183.678528      2.561745        21.394557        0.871452   \n",
       "14     175.311657     12.944425        21.336044        1.767870   \n",
       "15     178.296248      2.926505        21.630373        0.291683   \n",
       "16     172.294253     10.734698        22.667302        2.671143   \n",
       "17     173.066506      7.797931        22.042321        2.201104   \n",
       "18     160.185623      6.154684        22.120637        1.715451   \n",
       "19     154.716878      4.048059        23.099909        2.722425   \n",
       "20     160.146975      4.332940        21.622918        0.544349   \n",
       "21     141.816327      3.315259        23.385035        1.704255   \n",
       "22     140.778459      8.244477        23.098364        3.046290   \n",
       "23     144.068076      2.543079        21.405944        0.622739   \n",
       "24     140.789716      5.046037        23.505679        1.898376   \n",
       "25     141.560911     12.842808        20.430560        1.227738   \n",
       "26     109.830957      4.216425        12.254573        3.053849   \n",
       "\n",
       "   param_classifier__C param_classifier__max_iter param_classifier__n_jobs  \\\n",
       "0                    5                        100                       -1   \n",
       "1                    5                        500                       -1   \n",
       "2                    5                       1000                       -1   \n",
       "3                    2                        100                       -1   \n",
       "4                    2                        500                       -1   \n",
       "5                    2                       1000                       -1   \n",
       "6                    1                        100                       -1   \n",
       "7                    1                        500                       -1   \n",
       "8                    1                       1000                       -1   \n",
       "9                  0.5                        100                       -1   \n",
       "10                 0.5                        500                       -1   \n",
       "11                 0.5                       1000                       -1   \n",
       "12                 0.2                        100                       -1   \n",
       "13                 0.2                        500                       -1   \n",
       "14                 0.2                       1000                       -1   \n",
       "15                 0.1                        100                       -1   \n",
       "16                 0.1                        500                       -1   \n",
       "17                 0.1                       1000                       -1   \n",
       "18                0.05                        100                       -1   \n",
       "19                0.05                        500                       -1   \n",
       "20                0.05                       1000                       -1   \n",
       "21                0.02                        100                       -1   \n",
       "22                0.02                        500                       -1   \n",
       "23                0.02                       1000                       -1   \n",
       "24                0.01                        100                       -1   \n",
       "25                0.01                        500                       -1   \n",
       "26                0.01                       1000                       -1   \n",
       "\n",
       "   param_classifier__solver  \\\n",
       "0                 newton-cg   \n",
       "1                 newton-cg   \n",
       "2                 newton-cg   \n",
       "3                 newton-cg   \n",
       "4                 newton-cg   \n",
       "5                 newton-cg   \n",
       "6                 newton-cg   \n",
       "7                 newton-cg   \n",
       "8                 newton-cg   \n",
       "9                 newton-cg   \n",
       "10                newton-cg   \n",
       "11                newton-cg   \n",
       "12                newton-cg   \n",
       "13                newton-cg   \n",
       "14                newton-cg   \n",
       "15                newton-cg   \n",
       "16                newton-cg   \n",
       "17                newton-cg   \n",
       "18                newton-cg   \n",
       "19                newton-cg   \n",
       "20                newton-cg   \n",
       "21                newton-cg   \n",
       "22                newton-cg   \n",
       "23                newton-cg   \n",
       "24                newton-cg   \n",
       "25                newton-cg   \n",
       "26                newton-cg   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'classifier__C': 5, 'classifier__max_iter': 1...           0.579356   \n",
       "1   {'classifier__C': 5, 'classifier__max_iter': 5...           0.579356   \n",
       "2   {'classifier__C': 5, 'classifier__max_iter': 1...           0.579356   \n",
       "3   {'classifier__C': 2, 'classifier__max_iter': 1...           0.587253   \n",
       "4   {'classifier__C': 2, 'classifier__max_iter': 5...           0.587253   \n",
       "5   {'classifier__C': 2, 'classifier__max_iter': 1...           0.587253   \n",
       "6   {'classifier__C': 1, 'classifier__max_iter': 1...           0.595220   \n",
       "7   {'classifier__C': 1, 'classifier__max_iter': 5...           0.595220   \n",
       "8   {'classifier__C': 1, 'classifier__max_iter': 1...           0.595220   \n",
       "9   {'classifier__C': 0.5, 'classifier__max_iter':...           0.603816   \n",
       "10  {'classifier__C': 0.5, 'classifier__max_iter':...           0.603816   \n",
       "11  {'classifier__C': 0.5, 'classifier__max_iter':...           0.603816   \n",
       "12  {'classifier__C': 0.2, 'classifier__max_iter':...           0.619890   \n",
       "13  {'classifier__C': 0.2, 'classifier__max_iter':...           0.619890   \n",
       "14  {'classifier__C': 0.2, 'classifier__max_iter':...           0.619890   \n",
       "15  {'classifier__C': 0.1, 'classifier__max_iter':...           0.629045   \n",
       "16  {'classifier__C': 0.1, 'classifier__max_iter':...           0.629045   \n",
       "17  {'classifier__C': 0.1, 'classifier__max_iter':...           0.629045   \n",
       "18  {'classifier__C': 0.05, 'classifier__max_iter'...           0.632399   \n",
       "19  {'classifier__C': 0.05, 'classifier__max_iter'...           0.632399   \n",
       "20  {'classifier__C': 0.05, 'classifier__max_iter'...           0.632399   \n",
       "21  {'classifier__C': 0.02, 'classifier__max_iter'...           0.630372   \n",
       "22  {'classifier__C': 0.02, 'classifier__max_iter'...           0.630372   \n",
       "23  {'classifier__C': 0.02, 'classifier__max_iter'...           0.630372   \n",
       "24  {'classifier__C': 0.01, 'classifier__max_iter'...           0.627787   \n",
       "25  {'classifier__C': 0.01, 'classifier__max_iter'...           0.627787   \n",
       "26  {'classifier__C': 0.01, 'classifier__max_iter'...           0.627787   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.581173           0.590817           0.586735   \n",
       "1            0.581173           0.590817           0.586735   \n",
       "2            0.581173           0.590817           0.586735   \n",
       "3            0.587882           0.597666           0.594353   \n",
       "4            0.587882           0.597666           0.594353   \n",
       "5            0.587882           0.597666           0.594353   \n",
       "6            0.596478           0.602767           0.602041   \n",
       "7            0.596478           0.602767           0.602041   \n",
       "8            0.596478           0.602767           0.602041   \n",
       "9            0.608288           0.612831           0.613363   \n",
       "10           0.608288           0.612831           0.613363   \n",
       "11           0.608288           0.612831           0.613363   \n",
       "12           0.618632           0.626669           0.625245   \n",
       "13           0.618632           0.626669           0.625245   \n",
       "14           0.618632           0.626669           0.625245   \n",
       "15           0.625690           0.636732           0.632723   \n",
       "16           0.625690           0.636732           0.632723   \n",
       "17           0.625690           0.636732           0.632723   \n",
       "18           0.630163           0.639248           0.638454   \n",
       "19           0.630163           0.639248           0.638454   \n",
       "20           0.630163           0.639248           0.638454   \n",
       "21           0.627857           0.637291           0.638174   \n",
       "22           0.627857           0.637291           0.638174   \n",
       "23           0.627857           0.637291           0.638174   \n",
       "24           0.622405           0.631351           0.632793   \n",
       "25           0.622405           0.631351           0.632793   \n",
       "26           0.622405           0.631351           0.632793   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.590369         0.585690        0.004686               25  \n",
       "1            0.590369         0.585690        0.004686               25  \n",
       "2            0.590369         0.585690        0.004686               25  \n",
       "3            0.597638         0.592958        0.004568               22  \n",
       "4            0.597638         0.592958        0.004568               22  \n",
       "5            0.597638         0.592958        0.004568               22  \n",
       "6            0.604697         0.600241        0.003711               19  \n",
       "7            0.604697         0.600241        0.003711               19  \n",
       "8            0.604697         0.600241        0.003711               19  \n",
       "9            0.613573         0.610374        0.003810               16  \n",
       "10           0.613573         0.610374        0.003810               16  \n",
       "11           0.613573         0.610374        0.003810               16  \n",
       "12           0.625594         0.623206        0.003279               13  \n",
       "13           0.625594         0.623206        0.003279               13  \n",
       "14           0.625594         0.623206        0.003279               13  \n",
       "15           0.634121         0.631662        0.003883                7  \n",
       "16           0.634121         0.631662        0.003883                7  \n",
       "17           0.634121         0.631662        0.003883                7  \n",
       "18           0.638803         0.635814        0.003776                1  \n",
       "19           0.638803         0.635814        0.003776                1  \n",
       "20           0.638803         0.635814        0.003776                1  \n",
       "21           0.639922         0.634723        0.004725                4  \n",
       "22           0.639922         0.634723        0.004725                4  \n",
       "23           0.639922         0.634723        0.004725                4  \n",
       "24           0.633003         0.629468        0.003995               10  \n",
       "25           0.633003         0.629468        0.003995               10  \n",
       "26           0.633003         0.629468        0.003995               10  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(grid_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd032e-2505-4180-941e-aa55e34aa53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccd2b721",
   "metadata": {},
   "source": [
    "All the older stuff is below here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4f9117d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[    0     1     2 ... 71540 71541 71542]\n",
      "  Test:  index=[    7    10    21 ... 71512 71515 71520]\n",
      "model score: 0.569\n",
      "46.16732168197632\n",
      "Fold 1:\n",
      "  Train: index=[    0     1     2 ... 71540 71541 71542]\n",
      "  Test:  index=[    3     4    11 ... 71526 71530 71538]\n",
      "model score: 0.579\n",
      "43.28184628486633\n",
      "Fold 2:\n",
      "  Train: index=[    0     1     2 ... 71540 71541 71542]\n",
      "  Test:  index=[    8    16    30 ... 71504 71525 71527]\n",
      "model score: 0.578\n",
      "48.57866644859314\n",
      "Fold 3:\n",
      "  Train: index=[    0     1     2 ... 71539 71540 71542]\n",
      "  Test:  index=[   25    27    44 ... 71487 71505 71541]\n",
      "model score: 0.579\n",
      "43.54520773887634\n",
      "Fold 4:\n",
      "  Train: index=[    1     2     3 ... 71540 71541 71542]\n",
      "  Test:  index=[    0    12    15 ... 71536 71537 71539]\n",
      "model score: 0.568\n",
      "48.14018964767456\n",
      "Fold 5:\n",
      "  Train: index=[    0     1     2 ... 71539 71540 71541]\n",
      "  Test:  index=[   17    19    22 ... 71531 71535 71542]\n",
      "model score: 0.586\n",
      "50.04946684837341\n",
      "Fold 6:\n",
      "  Train: index=[    0     1     2 ... 71540 71541 71542]\n",
      "  Test:  index=[   14    20    35 ... 71497 71499 71521]\n",
      "model score: 0.583\n",
      "41.702008962631226\n",
      "Fold 7:\n",
      "  Train: index=[    0     1     2 ... 71539 71541 71542]\n",
      "  Test:  index=[    5     6    13 ... 71528 71532 71540]\n",
      "model score: 0.581\n",
      "55.94741368293762\n",
      "Fold 8:\n",
      "  Train: index=[    0     3     4 ... 71540 71541 71542]\n",
      "  Test:  index=[    1     2     9 ... 71514 71518 71533]\n",
      "model score: 0.571\n",
      "48.81972336769104\n",
      "Fold 9:\n",
      "  Train: index=[    0     1     2 ... 71540 71541 71542]\n",
      "  Test:  index=[   24    29    31 ... 71513 71524 71529]\n",
      "model score: 0.581\n",
      "52.01545572280884\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(review_features)):\n",
    "    start = time.time()\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "    x_train = review_features.loc[train_index, :]\n",
    "    x_test = review_features.loc[test_index, :]\n",
    "    y_train = np.ravel(y.loc[train_index, :])\n",
    "    y_test = np.ravel(y.loc[test_index,:])\n",
    "    \n",
    "    #x_train_features = wordbagger.fit_transform(x_train)\n",
    "    #x_test_features = wordbagger.fit_transform(x_test)\n",
    "    #scaler = preprocessing.MaxAbsScaler().fit(x_train_features)\n",
    "    #x_train_scaled = scaler.transform(x_train_features)\n",
    "    #print(type(y_train))\n",
    "    #print(x_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    #clf = Pipeline(steps = [(\"preprocess\", preprocessor), ('classifier', LogisticRegression()) ])\n",
    "    clf.fit(x_train, y_train)\n",
    "    #print(x_train)\n",
    "    #print(y_train.shape)\n",
    "    #X_trans = preprocessor.fit_transform(x_train)\n",
    "    #print(x_train.shape)\n",
    "    print(\"model score: %.3f\" % clf.score(x_test, y_test))\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bb63e",
   "metadata": {},
   "source": [
    "NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a38ef6d-1722-4684-9d96-93b6e5790cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I am being handed a list of documents\", \"Each of these documents has several unique words\", \"The words will represent the class of each review\", \"I am also removing stopwords in order to make this make more sense\"]\n",
    "cleaned_corpus = [transform_document(doc) for doc in corpus]\n",
    "vocabulary = vocabulary_from_corpus(cleaned_corpus, True)\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), \n",
    "                 ('tfid', TfidfTransformer())]).fit(cleaned_corpus)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c8e5cb-dfb0-4602-b800-3f6cc25f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    file = open('en.txt')\n",
    "    stopwords = []\n",
    "    for line in file:\n",
    "        stopwords.append(line.rstrip())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c4e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_corpus(review_group):\n",
    "    stopwords = get_stopwords()\n",
    "    reviewTextSet = review_group['reviewText']\n",
    "    for index in review_group.index:\n",
    "        curr_parsed = nlp(reviewTextSet[index].lower())\n",
    "        doclist = []\n",
    "        for token in curr_parsed:\n",
    "            lemma = token.lemma_\n",
    "            if not(re.match(\"[a-z0-9]+\", lemma)):\n",
    "                continue\n",
    "            if lemma not in stopwords:\n",
    "                doclist.append(lemma)\n",
    "        reviewTextSet[index] = \" \".join(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9a82e873-6a79-4d55-9c06-94041197a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_document(doc, remove_stopwords = True):\n",
    "    #new_doc = \"\"\n",
    "    stopwords = get_stopwords() # is this slow?\n",
    "    parsed_text = nlp(doc) # is this slow\n",
    "    doclist = []\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            doclist.append(lemma) # this is less slow?\n",
    "    return \" \".join(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "944f9414-c74e-4287-8756-74e82d279fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(review_text, remove_stopwords = True):\n",
    "    word_bag = {}\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(review_text)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            if lemma in word_bag:\n",
    "                word_bag[lemma] += 1\n",
    "            else:\n",
    "                word_bag[lemma] = 1\n",
    "    return word_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c054d665-1ac0-4e05-9826-b96a2bc4bb43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'the', 'second', 'third', 'be', 'and', 'this', 'one', 'document']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vocabulary_from_corpus(corpus, remove_stopwords = True):\n",
    "    vocab_set = set()\n",
    "    for document in corpus:\n",
    "        word_bag = bag_of_words(document, remove_stopwords)\n",
    "        for word in word_bag.keys():\n",
    "            vocab_set.add(word)\n",
    "    return list(vocab_set)\n",
    "vocabulary_from_corpus(['this is the first document', 'this document is the second document', 'and this is the third one', 'is this the first document'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da656ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/product_training.json')\n",
    "review_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/review_training.json')\n",
    "#len(product_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098d8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "teststring = \"First, you need to preprocess the raw text data. This may involve tasks like tokenizing the text (i.e., splitting it into individual words), removing stopwords, stemming or lemmatizing the words, and converting the text into a numerical format that can be used as input for the model. Then, you need to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance.\"\n",
    "#teststring.lower()\n",
    "parsed = nlp(teststring.lower())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
