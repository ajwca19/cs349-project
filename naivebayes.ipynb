{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a99b21-db3c-44bc-ba47-c72d4a4f235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5b12a9-5bbc-4884-9edd-f0a75da71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(path, test=False):\n",
    "    #create appropriate file path\n",
    "    if test == False:\n",
    "        pfilename = path + \"/product_training.json\"\n",
    "        rfilename = path + \"/review_training.json\"\n",
    "    else:\n",
    "        pfilename = path + \"/product_test.json\"\n",
    "        rfilename = path + \"/review_test.json\"\n",
    "    \n",
    "    #extract files as pandas dataframes\n",
    "    product_df = pd.read_json(pfilename)\n",
    "    review_df = pd.read_json(rfilename).drop_duplicates(subset=[\"reviewerID\", \"unixReviewTime\"], keep=\"first\")\n",
    "    \n",
    "    review_df.drop(columns=[\"reviewerID\",\"vote\", \"unixReviewTime\",\"reviewTime\",\"style\",\"reviewerName\",\"image\"], axis=1 ,inplace=True)\n",
    "    \n",
    "    review_df['reviewText'].fillna(\"\", inplace=True)\n",
    "    review_df['summary'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    group = review_df.groupby(\"asin\")\n",
    "    \n",
    "    review_group_df = pd.DataFrame(columns = ['asin', 'numReviews', 'percentVerified','reviewText','summaryText'])\n",
    "    \n",
    "    for asin, data in group:\n",
    "        verifiedCount = data['verified'].sum()\n",
    "        reviewCount = data['asin'].count()\n",
    "        percentVerified = verifiedCount / reviewCount\n",
    "        new_row = {'asin': asin, 'numReviews': reviewCount, 'percentVerified': percentVerified, 'reviewText': ' '.join(data['reviewText']), 'summaryText': ' '.join(data['summary'])} \n",
    "        review_group_df = review_group_df.append(new_row, ignore_index = True)\n",
    "        \n",
    "    return (review_group_df, product_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddaf67f-9958-4427-852f-120974d5c5a9",
   "metadata": {},
   "source": [
    "PSEUDOCODE FOR WHAT WE NEED TO DO WITH THE TRAINING DATA:\n",
    " - Saahir/Amy starts by putting all of the reviews for a product in one string. This string is the DOCUMENT for the reviews of a product\n",
    " - I will then take the corpus and transform it to remove stopwords, punctuation, and lemmatize everything. This is the true \"bag of words\".\n",
    " - Then, I use the cleaned text data to train the Naive Bayes classifier\n",
    " - When we get test data, first clean in the same way, then use it in the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c8e5cb-dfb0-4602-b800-3f6cc25f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    file = open('en.txt')\n",
    "    stopwords = []\n",
    "    for line in file:\n",
    "        stopwords.append(line.rstrip())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a82e873-6a79-4d55-9c06-94041197a3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocess raw text datum involve task tokenize text i.e. split individual word remove stopword stem lemmatize word convert text numerical format input model split datum training testing set training set train model testing set evaluate performance'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_document(doc, remove_stopwords = True):\n",
    "    new_doc = \"\"\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(doc)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            new_doc += lemma + \" \"\n",
    "    return new_doc.rstrip()\n",
    "\n",
    "transform_document(\"First, you need to preprocess the raw text data. This may involve tasks like tokenizing the text (i.e., splitting it into individual words), removing stopwords, stemming or lemmatizing the words, and converting the text into a numerical format that can be used as input for the model. Then, you need to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944f9414-c74e-4287-8756-74e82d279fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(review_text, remove_stopwords = True):\n",
    "    word_bag = {}\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(review_text)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            if lemma in word_bag:\n",
    "                word_bag[lemma] += 1\n",
    "            else:\n",
    "                word_bag[lemma] = 1\n",
    "    return word_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c054d665-1ac0-4e05-9826-b96a2bc4bb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'the', 'this', 'first', 'second', 'document', 'third', 'one', 'be']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vocabulary_from_corpus(corpus, remove_stopwords = True):\n",
    "    vocab_set = set()\n",
    "    for document in corpus:\n",
    "        word_bag = bag_of_words(document, remove_stopwords)\n",
    "        for word in word_bag.keys():\n",
    "            vocab_set.add(word)\n",
    "    return list(vocab_set)\n",
    "\n",
    "def transform_document(doc, remove_stopwords = True):\n",
    "    new_doc = \"\"\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(doc)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            new_doc += lemma + \" \"\n",
    "    return new_doc.rstrip()\n",
    "\n",
    "vocabulary_from_corpus(['this is the first document', 'this document is the second document', 'and this is the third one', 'is this the first document'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a38ef6d-1722-4684-9d96-93b6e5790cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I am being handed a list of documents\", \"Each of these documents has several unique words\", \"The words will represent the class of each review\", \"I am also removing stopwords in order to make this make more sense\"]\n",
    "cleaned_corpus = [transform_document(doc) for doc in corpus]\n",
    "vocabulary = vocabulary_from_corpus(cleaned_corpus, True)\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), \n",
    "                 ('tfid', TfidfTransformer())]).fit(cleaned_corpus)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c92fbe-6af3-432f-8a0a-fff1ea309c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "(review_group_df, product_df) = data_preprocessing(\"../devided_dataset_v2/CDs_and_Vinyl/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82583053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df.loc[product_df['asin'] == '0000B049F5B33CD310EB1AB236E20191', 'awesomeness'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a846d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_group_df['reviewText'][0]\n",
    "\n",
    "review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e32d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
