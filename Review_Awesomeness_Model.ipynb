{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a99b21-db3c-44bc-ba47-c72d4a4f235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76acec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [WinError\n",
      "[nltk_data]     10060] A connection attempt failed because the\n",
      "[nltk_data]     connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2f043db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca5b12a9-5bbc-4884-9edd-f0a75da71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(path, test=False):\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    #create appropriate file path\n",
    "    if test == False:\n",
    "        pfilename = path + \"/product_training.json\"\n",
    "        rfilename = path + \"/review_training.json\"\n",
    "    else:\n",
    "        pfilename = path + \"/product_test.json\"\n",
    "        rfilename = path + \"/review_test.json\"\n",
    "    \n",
    "    #extract files as pandas dataframes\n",
    "    product_df = pd.read_json(pfilename)\n",
    "    \n",
    "    review_df = pd.read_json(rfilename).drop_duplicates(subset=[\"reviewerID\", \"unixReviewTime\"], keep=\"first\")\n",
    "    ## 11.66 seconds to get to here\n",
    "    \n",
    "    review_df.drop(columns=[\"reviewerID\",\"vote\", \"unixReviewTime\",\"reviewTime\",\"style\",\"reviewerName\",\"image\"], axis=1 ,inplace=True)\n",
    "    \n",
    "    review_df['reviewText'].fillna(\"\", inplace=True)\n",
    "    review_df['summary'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    review_df.sort_values('asin', inplace = True)\n",
    "    product_df.sort_values('asin', inplace = True)\n",
    "    \n",
    "    group = review_df.groupby(\"asin\")\n",
    "    \n",
    "    #review_group_df = pd.DataFrame(columns = ['asin', 'numReviews', 'percentVerified','reviewText','summaryText', 'awesomeness'])\n",
    "    \n",
    "    # about the same amount of time to get to here\n",
    "    start_time = time.time()\n",
    "    datalist = []\n",
    "    count = 0\n",
    "    #awesome_pos = 0\n",
    "    for asin, data in group:\n",
    "        verifiedCount = data['verified'].sum()\n",
    "        reviewCount = data['asin'].count()\n",
    "        percentVerified = verifiedCount / reviewCount\n",
    "        if count == 0:\n",
    "            print(type(data['reviewText']))\n",
    "        reviewText = ' '.join(data['reviewText'])\n",
    "        #reviewText = ' '.join(transform_document(x) for x in data['reviewText'])\n",
    "        #summaryText = \"\"\n",
    "        summaryText = ' '.join(data['summary'])\n",
    "        #summaryText = ' '.join(transform_document(x) for x in data['summary'])\n",
    "        #reviewText = transform_document(' '.join(data['reviewText']))\n",
    "        #summaryText = transform_document(' '.join(data['summary']))\n",
    "        #awesomeness = 0\n",
    "        \n",
    "        #SENTIMENT ANALYSIS CHUNK\n",
    "        (rev_means, rev_stdevs) = sentiment_analysis(data['reviewText'])\n",
    "        (sum_means, sum_stdevs) = sentiment_analysis(data['summary'])\n",
    "        while (product_df['asin'][count] != asin):\n",
    "               count = count + 1\n",
    "        \n",
    "        awesomeness = product_df['awesomeness'][count]\n",
    "        #awesome_pos = awesome_pos + reviewCount\n",
    "        #awesomeness = product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0] #might be slow\n",
    "        datalist.append([asin,  reviewCount, percentVerified, reviewText, \\\n",
    "                         summaryText, rev_means[0], rev_means[1], rev_means[2], \\\n",
    "                         rev_stdevs[0], rev_stdevs[1], rev_stdevs[2], sum_means[0], \\\n",
    "                         sum_means[1], sum_means[2], sum_stdevs[0], sum_stdevs[1], \\\n",
    "                         sum_stdevs[2], awesomeness])\n",
    "        \n",
    "        count = count + 1\n",
    "        #if count > 100:\n",
    "        #    break\n",
    "        \n",
    "        '''new_row = {'asin': asin, \n",
    "                   'numReviews': reviewCount, \n",
    "                   'percentVerified': percentVerified, \n",
    "                   'reviewText': transform_document(' '.join(data['reviewText'])), \n",
    "                   'summaryText': transform_document(' '.join(data['summary'])), \n",
    "                   'awesomeness': product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0]} \n",
    "        review_group_df = review_group_df.append(new_row, ignore_index = True)\n",
    "         '''\n",
    "    review_group_df = pd.DataFrame(datalist,columns =['asin', 'numReviews', 'percentVerified','reviewText','summaryText', \\\n",
    "                                                      'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                                      'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                                      'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev', \\\n",
    "                                                      'awesomeness'])    \n",
    "    \n",
    "    review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data_new.json')\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    \n",
    "    return review_group_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c92fbe-6af3-432f-8a0a-fff1ea309c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed Data Generated (Reviews and Summaries aggregated, no NLP processing)\n",
    "#review_group_df = data_preprocessing(\"../devided_dataset_v2/CDs_and_Vinyl/train\")\n",
    "#review_group_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data.json')\n",
    "review_group_df = pd.read_json('cleaned_data_new.json')\n",
    "#review_group_df.to_json(\"preprocessed.json\")\n",
    "#review_group_df.head()\n",
    "#review_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_fast(path, test=False):\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    #create appropriate file path\n",
    "    if test == False:\n",
    "        pfilename = path + \"/product_training.json\"\n",
    "        rfilename = path + \"/review_training.json\"\n",
    "    else:\n",
    "        pfilename = path + \"/product_test.json\"\n",
    "        rfilename = path + \"/review_test.json\"\n",
    "    \n",
    "    #extract files as pandas dataframes\n",
    "    product_df = pd.read_json(pfilename)\n",
    "    \n",
    "    review_df = pd.read_json(rfilename).drop_duplicates(subset=[\"reviewerID\", \"unixReviewTime\"], keep=\"first\")\n",
    "    ## 11.66 seconds to get to here\n",
    "    \n",
    "    review_df.drop(columns=[\"reviewerID\",\"vote\", \"unixReviewTime\",\"reviewTime\",\"style\",\"reviewerName\",\"image\"], axis=1 ,inplace=True)\n",
    "    \n",
    "    review_df['reviewText'].fillna(\"\", inplace=True)\n",
    "    review_df['summary'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    review_df.sort_values('asin', inplace = True)\n",
    "    product_df.sort_values('asin', inplace = True)\n",
    "    \n",
    "    group = review_df.groupby(\"asin\")\n",
    "    \n",
    "    #review_group_df = pd.DataFrame(columns = ['asin', 'numReviews', 'percentVerified','reviewText','summaryText', 'awesomeness'])\n",
    "    \n",
    "    # about the same amount of time to get to here\n",
    "    start_time = time.time()\n",
    "    datalist = []\n",
    "    count = 0\n",
    "    #awesome_pos = 0\n",
    "    for asin, data in group:\n",
    "        verifiedCount = data['verified'].sum()\n",
    "        reviewCount = data['asin'].count()\n",
    "        percentVerified = verifiedCount / reviewCount\n",
    "        if count == 0:\n",
    "            print(type(data['reviewText']))\n",
    "        #reviewText = ' '.join(data['reviewText'])\n",
    "        #reviewText = ' '.join(transform_document(x) for x in data['reviewText'])\n",
    "        #summaryText = \"\"\n",
    "        #summaryText = ' '.join(data['summary'])\n",
    "        #summaryText = ' '.join(transform_document(x) for x in data['summary'])\n",
    "        #reviewText = transform_document(' '.join(data['reviewText']))\n",
    "        #summaryText = transform_document(' '.join(data['summary']))\n",
    "        #awesomeness = 0\n",
    "        \n",
    "        #SENTIMENT ANALYSIS CHUNK\n",
    "        (rev_means, rev_stdevs) = sentiment_analysis(data['reviewText'])\n",
    "        (sum_means, sum_stdevs) = sentiment_analysis(data['summary'])\n",
    "        while (product_df['asin'][count] != asin):\n",
    "               count = count + 1\n",
    "        \n",
    "        awesomeness = product_df['awesomeness'][count]\n",
    "        #awesome_pos = awesome_pos + reviewCount\n",
    "        #awesomeness = product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0] #might be slow\n",
    "        datalist.append([asin,  reviewCount, percentVerified, \\\n",
    "                         rev_means[0], rev_means[1], rev_means[2], \\\n",
    "                         rev_stdevs[0], rev_stdevs[1], rev_stdevs[2], sum_means[0], \\\n",
    "                         sum_means[1], sum_means[2], sum_stdevs[0], sum_stdevs[1], \\\n",
    "                         sum_stdevs[2], awesomeness])\n",
    "        \n",
    "        count = count + 1\n",
    "        #if count > 100:\n",
    "        #    break\n",
    "        \n",
    "        '''new_row = {'asin': asin, \n",
    "                   'numReviews': reviewCount, \n",
    "                   'percentVerified': percentVerified, \n",
    "                   'reviewText': transform_document(' '.join(data['reviewText'])), \n",
    "                   'summaryText': transform_document(' '.join(data['summary'])), \n",
    "                   'awesomeness': product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0]} \n",
    "        review_group_df = review_group_df.append(new_row, ignore_index = True)\n",
    "         '''\n",
    "    review_group_df = pd.DataFrame(datalist,columns =['asin', 'numReviews', 'percentVerified','reviewText','summaryText', \\\n",
    "                                                      'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                                      'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                                      'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev', \\\n",
    "                                                      'awesomeness'])    \n",
    "    \n",
    "    review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data_new.json')\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    \n",
    "    return review_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663aa55a-fe39-4c15-a6cb-27f4fe073e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(docs):\n",
    "    sentiments = []\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    for doc in docs:\n",
    "        polarities = sid.polarity_scores(doc)\n",
    "        sentiments.append((polarities['pos'], polarities['neu'], polarities['neg']))\n",
    "    if len(sentiments) == 1:\n",
    "        return (sentiments[0], (0, 0, 0))\n",
    "    else:\n",
    "        pos = [sents[0] for sents in sentiments]\n",
    "        neu = [sents[1] for sents in sentiments]\n",
    "        neg = [sents[2] for sents in sentiments]\n",
    "        return ((statistics.mean(pos), statistics.mean(neu), statistics.mean(neg)), (statistics.stdev(pos), statistics.stdev(neu), statistics.stdev(neg)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473a32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_transformer = Pipeline(\n",
    "    steps = [('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer())]\n",
    ")\n",
    "wordbagger = ColumnTransformer(\n",
    "    transformers=[(\"rev\", string_transformer, 'reviewText'), \n",
    "                  (\"sum\", string_transformer, 'summaryText')]\n",
    "    , remainder='passthrough'\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps = [(\"wordbag\", wordbagger), \n",
    "                        (\"scale\", MaxAbsScaler()), \n",
    "                        ('classifier', LogisticRegression(solver=\"newton-cg\", C=0.05, max_iter = 500, n_jobs=-1))])\n",
    "#x = review_group_df.filter(['numReviews', 'percentVerified', 'reviewText', 'summaryText'])\n",
    "#clf = LogisticRegression(tol = 0.001, max_iter = 150)\n",
    "#clf.fit(x,y)\n",
    "review_features = review_group_df.filter(['numReviews', 'percentVerified', 'reviewText', 'summaryText', \\\n",
    "                                          'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                          'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                          'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev'])\n",
    "fewer_features = review_group_df.filter(['numReviews', 'percentVerified', \\\n",
    "                                          'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                          'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                          'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev'])\n",
    "y = review_group_df.filter(['awesomeness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4156965",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(steps = [(\"wordbag\", wordbagger), \n",
    "                                  (\"scale\", MaxAbsScaler())])\n",
    "processed_features = preprocessor.fit_transform(review_features)\n",
    "smaller_processed_features = MaxAbsScaler().fit_transform(fewer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "64b1e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_clf = SelectFromModel(threshold = \"1.5*mean\", \\\n",
    "                             estimator=LogisticRegression(solver=\"newton-cg\", C=0.05, max_iter = 500, n_jobs=-1))\n",
    "log_clf = LogisticRegression(solver=\"newton-cg\", C=0.05, max_iter = 500, n_jobs=-1)\n",
    "multi_clf = MultinomialNB()\n",
    "random_forest_clf = RandomForestClassifier(max_depth = 150, n_jobs = -1)\n",
    "boosted_clf = GradientBoostingClassifier()\n",
    "#just_clf.fit(processed_features, y)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('RF', random_forest_clf), ('mN', multi_clf), ('LR', log_clf)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c7a6713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I started at 1683787430.7084897\n",
      "28.527466130256652\n"
     ]
    }
   ],
   "source": [
    "var_processed_features = VarianceThreshold().fit_transform(processed_features)\n",
    "more_processed_features = select_clf.fit_transform(var_processed_features, np.ravel(y))\n",
    "start = time.time()\n",
    "print(\"I started at \" + str(start))\n",
    "# this runs the k-fold cross-validation automatically?\n",
    "cv10_results = cross_validate(log_clf, more_processed_features, np.ravel(y), cv=10, n_jobs = -1, scoring = ['f1_macro', 'precision', 'recall'])\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9f2d2011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6215531070176024\n",
      "0.6304416111410547\n",
      "0.7316062637253422\n"
     ]
    }
   ],
   "source": [
    "#print(cv10_results)\n",
    "#print(cv10_results)\n",
    "print(statistics.mean(cv10_results['test_f1_macro']))\n",
    "#print(statistics.stdev(cv10_results['test_f1_macro']))\n",
    "print(statistics.mean(cv10_results['test_precision']))\n",
    "#print(statistics.stdev(cv10_results['test_precision']))\n",
    "print(statistics.mean(cv10_results['test_recall']))\n",
    "#print(statistics.stdev(cv10_results['test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e33a3240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71543x366180 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28605864 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4475ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71543x366180 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28605864 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5d753f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71543x69395 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21945286 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f9117d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[    0     1     2 ... 71540 71541 71542]\n",
      "  Test:  index=[    4    20    22 ... 71508 71520 71523]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 109762 features per sample; expecting 115607",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13680/463239938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mlog_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_selected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_proc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;31m#print(x_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#print(y_train.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0m\u001b[0;32m    289\u001b[0m                              % (X.shape[1], n_features))\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 109762 features per sample; expecting 115607"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(review_features)):\n",
    "    start = time.time()\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "    x_train = review_features.loc[train_index, :]\n",
    "    x_test = review_features.loc[test_index, :]\n",
    "    y_train = np.ravel(y.loc[train_index, :])\n",
    "    y_test = np.ravel(y.loc[test_index,:])\n",
    "    \n",
    "    x_train_proc = preprocessor.fit_transform(x_train)\n",
    "    x_test_proc = preprocessor.fit_transform(x_test)\n",
    "    \n",
    "    #x_train_features = wordbagger.fit_transform(x_train)\n",
    "    #x_test_features = wordbagger.fit_transform(x_test)\n",
    "    #scaler = preprocessing.MaxAbsScaler().fit(x_train_features)\n",
    "    #x_train_scaled = scaler.transform(x_train_features)\n",
    "    #print(type(y_train))\n",
    "    #print(x_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    #clf = Pipeline(steps = [(\"preprocess\", preprocessor), ('classifier', LogisticRegression()) ])\n",
    "    x_train_selected = select_clf.fit_transform(x_train_proc, y_train)\n",
    "    x_test_selected = select_clf.fit_transform(x_test_proc, y_test)\n",
    "    log_clf.fit(x_train_selected, y_train)\n",
    "    \n",
    "    y_pred = log_clf.predict(x_test_proc)\n",
    "    #print(x_train)\n",
    "    #print(y_train.shape)\n",
    "    #X_trans = preprocessor.fit_transform(x_train)\n",
    "    #print(x_train.shape)\n",
    "    print(\"model score: %.3f\" % f1_score(y_test, y_pred, 'macro'))\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91880e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d13e321a-2bf9-47f3-a4a6-c9dd1e4aec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm starting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.023726880550385\n"
     ]
    }
   ],
   "source": [
    "solvers = ['newton-cg']\n",
    "jobs = [-1]\n",
    "maxiter = [100, 500, 1000]\n",
    "c = [5, 2, 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01]\n",
    "param_grid={'classifier__C':c, 'classifier__solver':solvers, 'classifier__n_jobs':jobs, 'classifier__max_iter':maxiter}\n",
    "grid_clf = GridSearchCV(clf, param_grid, n_jobs=-1)\n",
    "start = time.time()\n",
    "print(\"I'm starting!\")\n",
    "grid_clf.fit(review_features, np.ravel(y))\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cea6b793-832b-4a98-bb2b-95193533a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "562fb612-31e0-43a7-9a47-cca8d896d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__max_iter</th>\n",
       "      <th>param_classifier__n_jobs</th>\n",
       "      <th>param_classifier__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.902761</td>\n",
       "      <td>3.981755</td>\n",
       "      <td>22.498697</td>\n",
       "      <td>0.715377</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.581173</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.586735</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263.848393</td>\n",
       "      <td>8.369911</td>\n",
       "      <td>22.481720</td>\n",
       "      <td>1.092433</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__max_iter': 5...</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.581173</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.586735</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258.521113</td>\n",
       "      <td>0.950623</td>\n",
       "      <td>21.237789</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.581173</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.586735</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>242.788301</td>\n",
       "      <td>6.899337</td>\n",
       "      <td>26.501668</td>\n",
       "      <td>2.905537</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.587253</td>\n",
       "      <td>0.587882</td>\n",
       "      <td>0.597666</td>\n",
       "      <td>0.594353</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.592958</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>245.064487</td>\n",
       "      <td>12.992323</td>\n",
       "      <td>22.550827</td>\n",
       "      <td>1.717254</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__max_iter': 5...</td>\n",
       "      <td>0.587253</td>\n",
       "      <td>0.587882</td>\n",
       "      <td>0.597666</td>\n",
       "      <td>0.594353</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.592958</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>230.603351</td>\n",
       "      <td>2.580260</td>\n",
       "      <td>22.640238</td>\n",
       "      <td>0.726609</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.587253</td>\n",
       "      <td>0.587882</td>\n",
       "      <td>0.597666</td>\n",
       "      <td>0.594353</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.592958</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>217.788828</td>\n",
       "      <td>4.732246</td>\n",
       "      <td>23.282278</td>\n",
       "      <td>1.899143</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.596478</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.604697</td>\n",
       "      <td>0.600241</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>219.208971</td>\n",
       "      <td>2.917052</td>\n",
       "      <td>21.258987</td>\n",
       "      <td>0.611024</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__max_iter': 5...</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.596478</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.604697</td>\n",
       "      <td>0.600241</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>214.697790</td>\n",
       "      <td>8.029622</td>\n",
       "      <td>23.524169</td>\n",
       "      <td>2.573233</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.596478</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.604697</td>\n",
       "      <td>0.600241</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202.574223</td>\n",
       "      <td>15.012335</td>\n",
       "      <td>22.778429</td>\n",
       "      <td>3.025260</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__max_iter':...</td>\n",
       "      <td>0.603816</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.613573</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>203.391861</td>\n",
       "      <td>2.496271</td>\n",
       "      <td>21.045616</td>\n",
       "      <td>0.183373</td>\n",
       "      <td>0.5</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__max_iter':...</td>\n",
       "      <td>0.603816</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.613573</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>193.466252</td>\n",
       "      <td>13.984599</td>\n",
       "      <td>22.463085</td>\n",
       "      <td>2.696157</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__max_iter':...</td>\n",
       "      <td>0.603816</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.613573</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>182.137830</td>\n",
       "      <td>19.212568</td>\n",
       "      <td>21.655008</td>\n",
       "      <td>1.693563</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__max_iter':...</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.626669</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>183.678528</td>\n",
       "      <td>2.561745</td>\n",
       "      <td>21.394557</td>\n",
       "      <td>0.871452</td>\n",
       "      <td>0.2</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__max_iter':...</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.626669</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>175.311657</td>\n",
       "      <td>12.944425</td>\n",
       "      <td>21.336044</td>\n",
       "      <td>1.767870</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__max_iter':...</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.626669</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>178.296248</td>\n",
       "      <td>2.926505</td>\n",
       "      <td>21.630373</td>\n",
       "      <td>0.291683</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__max_iter':...</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.625690</td>\n",
       "      <td>0.636732</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.631662</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>172.294253</td>\n",
       "      <td>10.734698</td>\n",
       "      <td>22.667302</td>\n",
       "      <td>2.671143</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__max_iter':...</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.625690</td>\n",
       "      <td>0.636732</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.631662</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>173.066506</td>\n",
       "      <td>7.797931</td>\n",
       "      <td>22.042321</td>\n",
       "      <td>2.201104</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__max_iter':...</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.625690</td>\n",
       "      <td>0.636732</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.631662</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>160.185623</td>\n",
       "      <td>6.154684</td>\n",
       "      <td>22.120637</td>\n",
       "      <td>1.715451</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.05, 'classifier__max_iter'...</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>0.638454</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.635814</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>154.716878</td>\n",
       "      <td>4.048059</td>\n",
       "      <td>23.099909</td>\n",
       "      <td>2.722425</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.05, 'classifier__max_iter'...</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>0.638454</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.635814</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>160.146975</td>\n",
       "      <td>4.332940</td>\n",
       "      <td>21.622918</td>\n",
       "      <td>0.544349</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.05, 'classifier__max_iter'...</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>0.638454</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.635814</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>141.816327</td>\n",
       "      <td>3.315259</td>\n",
       "      <td>23.385035</td>\n",
       "      <td>1.704255</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.02, 'classifier__max_iter'...</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>140.778459</td>\n",
       "      <td>8.244477</td>\n",
       "      <td>23.098364</td>\n",
       "      <td>3.046290</td>\n",
       "      <td>0.02</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.02, 'classifier__max_iter'...</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>144.068076</td>\n",
       "      <td>2.543079</td>\n",
       "      <td>21.405944</td>\n",
       "      <td>0.622739</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.02, 'classifier__max_iter'...</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>140.789716</td>\n",
       "      <td>5.046037</td>\n",
       "      <td>23.505679</td>\n",
       "      <td>1.898376</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__max_iter'...</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>0.622405</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.632793</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>141.560911</td>\n",
       "      <td>12.842808</td>\n",
       "      <td>20.430560</td>\n",
       "      <td>1.227738</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__max_iter'...</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>0.622405</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.632793</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>109.830957</td>\n",
       "      <td>4.216425</td>\n",
       "      <td>12.254573</td>\n",
       "      <td>3.053849</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__max_iter'...</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>0.622405</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.632793</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      265.902761      3.981755        22.498697        0.715377   \n",
       "1      263.848393      8.369911        22.481720        1.092433   \n",
       "2      258.521113      0.950623        21.237789        0.196200   \n",
       "3      242.788301      6.899337        26.501668        2.905537   \n",
       "4      245.064487     12.992323        22.550827        1.717254   \n",
       "5      230.603351      2.580260        22.640238        0.726609   \n",
       "6      217.788828      4.732246        23.282278        1.899143   \n",
       "7      219.208971      2.917052        21.258987        0.611024   \n",
       "8      214.697790      8.029622        23.524169        2.573233   \n",
       "9      202.574223     15.012335        22.778429        3.025260   \n",
       "10     203.391861      2.496271        21.045616        0.183373   \n",
       "11     193.466252     13.984599        22.463085        2.696157   \n",
       "12     182.137830     19.212568        21.655008        1.693563   \n",
       "13     183.678528      2.561745        21.394557        0.871452   \n",
       "14     175.311657     12.944425        21.336044        1.767870   \n",
       "15     178.296248      2.926505        21.630373        0.291683   \n",
       "16     172.294253     10.734698        22.667302        2.671143   \n",
       "17     173.066506      7.797931        22.042321        2.201104   \n",
       "18     160.185623      6.154684        22.120637        1.715451   \n",
       "19     154.716878      4.048059        23.099909        2.722425   \n",
       "20     160.146975      4.332940        21.622918        0.544349   \n",
       "21     141.816327      3.315259        23.385035        1.704255   \n",
       "22     140.778459      8.244477        23.098364        3.046290   \n",
       "23     144.068076      2.543079        21.405944        0.622739   \n",
       "24     140.789716      5.046037        23.505679        1.898376   \n",
       "25     141.560911     12.842808        20.430560        1.227738   \n",
       "26     109.830957      4.216425        12.254573        3.053849   \n",
       "\n",
       "   param_classifier__C param_classifier__max_iter param_classifier__n_jobs  \\\n",
       "0                    5                        100                       -1   \n",
       "1                    5                        500                       -1   \n",
       "2                    5                       1000                       -1   \n",
       "3                    2                        100                       -1   \n",
       "4                    2                        500                       -1   \n",
       "5                    2                       1000                       -1   \n",
       "6                    1                        100                       -1   \n",
       "7                    1                        500                       -1   \n",
       "8                    1                       1000                       -1   \n",
       "9                  0.5                        100                       -1   \n",
       "10                 0.5                        500                       -1   \n",
       "11                 0.5                       1000                       -1   \n",
       "12                 0.2                        100                       -1   \n",
       "13                 0.2                        500                       -1   \n",
       "14                 0.2                       1000                       -1   \n",
       "15                 0.1                        100                       -1   \n",
       "16                 0.1                        500                       -1   \n",
       "17                 0.1                       1000                       -1   \n",
       "18                0.05                        100                       -1   \n",
       "19                0.05                        500                       -1   \n",
       "20                0.05                       1000                       -1   \n",
       "21                0.02                        100                       -1   \n",
       "22                0.02                        500                       -1   \n",
       "23                0.02                       1000                       -1   \n",
       "24                0.01                        100                       -1   \n",
       "25                0.01                        500                       -1   \n",
       "26                0.01                       1000                       -1   \n",
       "\n",
       "   param_classifier__solver  \\\n",
       "0                 newton-cg   \n",
       "1                 newton-cg   \n",
       "2                 newton-cg   \n",
       "3                 newton-cg   \n",
       "4                 newton-cg   \n",
       "5                 newton-cg   \n",
       "6                 newton-cg   \n",
       "7                 newton-cg   \n",
       "8                 newton-cg   \n",
       "9                 newton-cg   \n",
       "10                newton-cg   \n",
       "11                newton-cg   \n",
       "12                newton-cg   \n",
       "13                newton-cg   \n",
       "14                newton-cg   \n",
       "15                newton-cg   \n",
       "16                newton-cg   \n",
       "17                newton-cg   \n",
       "18                newton-cg   \n",
       "19                newton-cg   \n",
       "20                newton-cg   \n",
       "21                newton-cg   \n",
       "22                newton-cg   \n",
       "23                newton-cg   \n",
       "24                newton-cg   \n",
       "25                newton-cg   \n",
       "26                newton-cg   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'classifier__C': 5, 'classifier__max_iter': 1...           0.579356   \n",
       "1   {'classifier__C': 5, 'classifier__max_iter': 5...           0.579356   \n",
       "2   {'classifier__C': 5, 'classifier__max_iter': 1...           0.579356   \n",
       "3   {'classifier__C': 2, 'classifier__max_iter': 1...           0.587253   \n",
       "4   {'classifier__C': 2, 'classifier__max_iter': 5...           0.587253   \n",
       "5   {'classifier__C': 2, 'classifier__max_iter': 1...           0.587253   \n",
       "6   {'classifier__C': 1, 'classifier__max_iter': 1...           0.595220   \n",
       "7   {'classifier__C': 1, 'classifier__max_iter': 5...           0.595220   \n",
       "8   {'classifier__C': 1, 'classifier__max_iter': 1...           0.595220   \n",
       "9   {'classifier__C': 0.5, 'classifier__max_iter':...           0.603816   \n",
       "10  {'classifier__C': 0.5, 'classifier__max_iter':...           0.603816   \n",
       "11  {'classifier__C': 0.5, 'classifier__max_iter':...           0.603816   \n",
       "12  {'classifier__C': 0.2, 'classifier__max_iter':...           0.619890   \n",
       "13  {'classifier__C': 0.2, 'classifier__max_iter':...           0.619890   \n",
       "14  {'classifier__C': 0.2, 'classifier__max_iter':...           0.619890   \n",
       "15  {'classifier__C': 0.1, 'classifier__max_iter':...           0.629045   \n",
       "16  {'classifier__C': 0.1, 'classifier__max_iter':...           0.629045   \n",
       "17  {'classifier__C': 0.1, 'classifier__max_iter':...           0.629045   \n",
       "18  {'classifier__C': 0.05, 'classifier__max_iter'...           0.632399   \n",
       "19  {'classifier__C': 0.05, 'classifier__max_iter'...           0.632399   \n",
       "20  {'classifier__C': 0.05, 'classifier__max_iter'...           0.632399   \n",
       "21  {'classifier__C': 0.02, 'classifier__max_iter'...           0.630372   \n",
       "22  {'classifier__C': 0.02, 'classifier__max_iter'...           0.630372   \n",
       "23  {'classifier__C': 0.02, 'classifier__max_iter'...           0.630372   \n",
       "24  {'classifier__C': 0.01, 'classifier__max_iter'...           0.627787   \n",
       "25  {'classifier__C': 0.01, 'classifier__max_iter'...           0.627787   \n",
       "26  {'classifier__C': 0.01, 'classifier__max_iter'...           0.627787   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.581173           0.590817           0.586735   \n",
       "1            0.581173           0.590817           0.586735   \n",
       "2            0.581173           0.590817           0.586735   \n",
       "3            0.587882           0.597666           0.594353   \n",
       "4            0.587882           0.597666           0.594353   \n",
       "5            0.587882           0.597666           0.594353   \n",
       "6            0.596478           0.602767           0.602041   \n",
       "7            0.596478           0.602767           0.602041   \n",
       "8            0.596478           0.602767           0.602041   \n",
       "9            0.608288           0.612831           0.613363   \n",
       "10           0.608288           0.612831           0.613363   \n",
       "11           0.608288           0.612831           0.613363   \n",
       "12           0.618632           0.626669           0.625245   \n",
       "13           0.618632           0.626669           0.625245   \n",
       "14           0.618632           0.626669           0.625245   \n",
       "15           0.625690           0.636732           0.632723   \n",
       "16           0.625690           0.636732           0.632723   \n",
       "17           0.625690           0.636732           0.632723   \n",
       "18           0.630163           0.639248           0.638454   \n",
       "19           0.630163           0.639248           0.638454   \n",
       "20           0.630163           0.639248           0.638454   \n",
       "21           0.627857           0.637291           0.638174   \n",
       "22           0.627857           0.637291           0.638174   \n",
       "23           0.627857           0.637291           0.638174   \n",
       "24           0.622405           0.631351           0.632793   \n",
       "25           0.622405           0.631351           0.632793   \n",
       "26           0.622405           0.631351           0.632793   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.590369         0.585690        0.004686               25  \n",
       "1            0.590369         0.585690        0.004686               25  \n",
       "2            0.590369         0.585690        0.004686               25  \n",
       "3            0.597638         0.592958        0.004568               22  \n",
       "4            0.597638         0.592958        0.004568               22  \n",
       "5            0.597638         0.592958        0.004568               22  \n",
       "6            0.604697         0.600241        0.003711               19  \n",
       "7            0.604697         0.600241        0.003711               19  \n",
       "8            0.604697         0.600241        0.003711               19  \n",
       "9            0.613573         0.610374        0.003810               16  \n",
       "10           0.613573         0.610374        0.003810               16  \n",
       "11           0.613573         0.610374        0.003810               16  \n",
       "12           0.625594         0.623206        0.003279               13  \n",
       "13           0.625594         0.623206        0.003279               13  \n",
       "14           0.625594         0.623206        0.003279               13  \n",
       "15           0.634121         0.631662        0.003883                7  \n",
       "16           0.634121         0.631662        0.003883                7  \n",
       "17           0.634121         0.631662        0.003883                7  \n",
       "18           0.638803         0.635814        0.003776                1  \n",
       "19           0.638803         0.635814        0.003776                1  \n",
       "20           0.638803         0.635814        0.003776                1  \n",
       "21           0.639922         0.634723        0.004725                4  \n",
       "22           0.639922         0.634723        0.004725                4  \n",
       "23           0.639922         0.634723        0.004725                4  \n",
       "24           0.633003         0.629468        0.003995               10  \n",
       "25           0.633003         0.629468        0.003995               10  \n",
       "26           0.633003         0.629468        0.003995               10  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(grid_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd032e-2505-4180-941e-aa55e34aa53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccd2b721",
   "metadata": {},
   "source": [
    "All the older stuff is below here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bb63e",
   "metadata": {},
   "source": [
    "NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a38ef6d-1722-4684-9d96-93b6e5790cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I am being handed a list of documents\", \"Each of these documents has several unique words\", \"The words will represent the class of each review\", \"I am also removing stopwords in order to make this make more sense\"]\n",
    "cleaned_corpus = [transform_document(doc) for doc in corpus]\n",
    "vocabulary = vocabulary_from_corpus(cleaned_corpus, True)\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), \n",
    "                 ('tfid', TfidfTransformer())]).fit(cleaned_corpus)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c8e5cb-dfb0-4602-b800-3f6cc25f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    file = open('en.txt')\n",
    "    stopwords = []\n",
    "    for line in file:\n",
    "        stopwords.append(line.rstrip())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c4e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_corpus(review_group):\n",
    "    stopwords = get_stopwords()\n",
    "    reviewTextSet = review_group['reviewText']\n",
    "    for index in review_group.index:\n",
    "        curr_parsed = nlp(reviewTextSet[index].lower())\n",
    "        doclist = []\n",
    "        for token in curr_parsed:\n",
    "            lemma = token.lemma_\n",
    "            if not(re.match(\"[a-z0-9]+\", lemma)):\n",
    "                continue\n",
    "            if lemma not in stopwords:\n",
    "                doclist.append(lemma)\n",
    "        reviewTextSet[index] = \" \".join(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9a82e873-6a79-4d55-9c06-94041197a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_document(doc, remove_stopwords = True):\n",
    "    #new_doc = \"\"\n",
    "    stopwords = get_stopwords() # is this slow?\n",
    "    parsed_text = nlp(doc) # is this slow\n",
    "    doclist = []\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            doclist.append(lemma) # this is less slow?\n",
    "    return \" \".join(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "944f9414-c74e-4287-8756-74e82d279fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(review_text, remove_stopwords = True):\n",
    "    word_bag = {}\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(review_text)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            if lemma in word_bag:\n",
    "                word_bag[lemma] += 1\n",
    "            else:\n",
    "                word_bag[lemma] = 1\n",
    "    return word_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c054d665-1ac0-4e05-9826-b96a2bc4bb43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'the', 'second', 'third', 'be', 'and', 'this', 'one', 'document']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vocabulary_from_corpus(corpus, remove_stopwords = True):\n",
    "    vocab_set = set()\n",
    "    for document in corpus:\n",
    "        word_bag = bag_of_words(document, remove_stopwords)\n",
    "        for word in word_bag.keys():\n",
    "            vocab_set.add(word)\n",
    "    return list(vocab_set)\n",
    "vocabulary_from_corpus(['this is the first document', 'this document is the second document', 'and this is the third one', 'is this the first document'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da656ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/product_training.json')\n",
    "review_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/review_training.json')\n",
    "#len(product_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098d8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "teststring = \"First, you need to preprocess the raw text data. This may involve tasks like tokenizing the text (i.e., splitting it into individual words), removing stopwords, stemming or lemmatizing the words, and converting the text into a numerical format that can be used as input for the model. Then, you need to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance.\"\n",
    "#teststring.lower()\n",
    "parsed = nlp(teststring.lower())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
