{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a99b21-db3c-44bc-ba47-c72d4a4f235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76acec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Saahir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "b2f043db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold, SelectPercentile, chi2, SelectKBest\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fd8b6576",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3936/3507038207.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python._version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ca5b12a9-5bbc-4884-9edd-f0a75da71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(path, test=False):\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    #create appropriate file path\n",
    "    if test == False:\n",
    "        pfilename = path + \"/product_training.json\"\n",
    "        rfilename = path + \"/review_training.json\"\n",
    "    else:\n",
    "        pfilename = path + \"/product_test.json\"\n",
    "        rfilename = path + \"/review_test.json\"\n",
    "    \n",
    "    #extract files as pandas dataframes\n",
    "    product_df = pd.read_json(pfilename)\n",
    "    \n",
    "    review_df = pd.read_json(rfilename).drop_duplicates(subset=[\"reviewerID\", \"unixReviewTime\", \"asin\"], keep=\"first\")\n",
    "    ## 11.66 seconds to get to here\n",
    "    \n",
    "    review_df.drop(columns=[\"reviewerID\",\"vote\", \"unixReviewTime\",\"reviewTime\",\"style\",\"reviewerName\",\"image\"], axis=1 ,inplace=True)\n",
    "    \n",
    "    review_df['reviewText'].fillna(\"\", inplace=True)\n",
    "    review_df['summary'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    review_df.sort_values('asin', inplace = True)\n",
    "    product_df.sort_values('asin', inplace = True)\n",
    "    \n",
    "    group = review_df.groupby(\"asin\")\n",
    "    \n",
    "    #review_group_df = pd.DataFrame(columns = ['asin', 'numReviews', 'percentVerified','reviewText','summaryText', 'awesomeness'])\n",
    "    \n",
    "    # about the same amount of time to get to here\n",
    "    start_time = time.time()\n",
    "    datalist = []\n",
    "    count = 0\n",
    "    #awesome_pos = 0\n",
    "    for asin, data in group:\n",
    "        verifiedCount = data['verified'].sum()\n",
    "        reviewCount = data['asin'].count()\n",
    "        percentVerified = verifiedCount / reviewCount\n",
    "        if count == 0:\n",
    "            print(type(data['reviewText']))\n",
    "        reviewText = ' '.join(data['reviewText'])\n",
    "        #reviewText = ' '.join(transform_document(x) for x in data['reviewText'])\n",
    "        #summaryText = \"\"\n",
    "        summaryText = ' '.join(data['summary'])\n",
    "        #summaryText = ' '.join(transform_document(x) for x in data['summary'])\n",
    "        #reviewText = transform_document(' '.join(data['reviewText']))\n",
    "        #summaryText = transform_document(' '.join(data['summary']))\n",
    "        #awesomeness = 0\n",
    "        \n",
    "        #SENTIMENT ANALYSIS CHUNK\n",
    "        (rev_means, rev_stdevs) = sentiment_analysis(data['reviewText'])\n",
    "        (sum_means, sum_stdevs) = sentiment_analysis(data['summary'])\n",
    "        while (product_df['asin'][count] != asin):\n",
    "               count = count + 1\n",
    "        \n",
    "        if test:\n",
    "            datalist.append([asin,  reviewCount, percentVerified, reviewText, \\\n",
    "                         summaryText, rev_means[0], rev_means[1], rev_means[2], \\\n",
    "                         rev_stdevs[0], rev_stdevs[1], rev_stdevs[2], sum_means[0], \\\n",
    "                         sum_means[1], sum_means[2], sum_stdevs[0], sum_stdevs[1], \\\n",
    "                         sum_stdevs[2]])\n",
    "        else:\n",
    "            awesomeness = product_df['awesomeness'][count]\n",
    "            #awesome_pos = awesome_pos + reviewCount\n",
    "            #awesomeness = product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0] #might be slow\n",
    "            datalist.append([asin,  reviewCount, percentVerified, reviewText, \\\n",
    "                         summaryText, rev_means[0], rev_means[1], rev_means[2], \\\n",
    "                         rev_stdevs[0], rev_stdevs[1], rev_stdevs[2], sum_means[0], \\\n",
    "                         sum_means[1], sum_means[2], sum_stdevs[0], sum_stdevs[1], \\\n",
    "                         sum_stdevs[2], awesomeness])\n",
    "        \n",
    "        count = count + 1\n",
    "        #if count > 100:\n",
    "        #    break\n",
    "        \n",
    "        '''new_row = {'asin': asin, \n",
    "                   'numReviews': reviewCount, \n",
    "                   'percentVerified': percentVerified, \n",
    "                   'reviewText': transform_document(' '.join(data['reviewText'])), \n",
    "                   'summaryText': transform_document(' '.join(data['summary'])), \n",
    "                   'awesomeness': product_df.loc[product_df['asin'] == asin, 'awesomeness'].values[0]} \n",
    "        review_group_df = review_group_df.append(new_row, ignore_index = True)\n",
    "         '''\n",
    "    if test:\n",
    "        review_group_df = pd.DataFrame(datalist,columns =['asin', 'numReviews', 'percentVerified','reviewText','summaryText', \\\n",
    "                                                      'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                                      'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                                      'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev'])    \n",
    "    else:\n",
    "        review_group_df = pd.DataFrame(datalist,columns =['asin', 'numReviews', 'percentVerified','reviewText','summaryText', \\\n",
    "                                                      'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                                      'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                                      'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev', \\\n",
    "                                                      'awesomeness'])    \n",
    "                                                          \n",
    "    if test:\n",
    "        review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data_new_test.json')\n",
    "    else:\n",
    "        review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data_new.json')\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    \n",
    "    return review_group_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "663aa55a-fe39-4c15-a6cb-27f4fe073e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(docs):\n",
    "    sentiments = []\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    for doc in docs:\n",
    "        polarities = sid.polarity_scores(doc)\n",
    "        sentiments.append((polarities['pos'], polarities['neu'], polarities['neg']))\n",
    "    if len(sentiments) == 1:\n",
    "        return (sentiments[0], (0, 0, 0))\n",
    "    else:\n",
    "        pos = [sents[0] for sents in sentiments]\n",
    "        neu = [sents[1] for sents in sentiments]\n",
    "        neg = [sents[2] for sents in sentiments]\n",
    "        return ((statistics.mean(pos), statistics.mean(neu), statistics.mean(neg)), (statistics.stdev(pos), statistics.stdev(neu), statistics.stdev(neg)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c92fbe-6af3-432f-8a0a-fff1ea309c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed Data Generated (Reviews and Summaries aggregated, no NLP processing)\n",
    "#review_group_df = data_preprocessing(\"../devided_dataset_v2/CDs_and_Vinyl/train\")\n",
    "#review_group_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data.json')\n",
    "review_group_df = pd.read_json('cleaned_data_new.json')\n",
    "#review_group_df.to_json(\"preprocessed.json\")\n",
    "#review_group_df.head()\n",
    "#review_group_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a06425c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>numReviews</th>\n",
       "      <th>percentVerified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summaryText</th>\n",
       "      <th>reviewPosMean</th>\n",
       "      <th>reviewNeuMean</th>\n",
       "      <th>reviewNegMean</th>\n",
       "      <th>reviewPosStDev</th>\n",
       "      <th>reviewNeuStDev</th>\n",
       "      <th>reviewNegStDev</th>\n",
       "      <th>summaryPosMean</th>\n",
       "      <th>summaryNeuMean</th>\n",
       "      <th>summaryNegMean</th>\n",
       "      <th>summaryPosStDev</th>\n",
       "      <th>summaryNeuStDev</th>\n",
       "      <th>summaryNegStDev</th>\n",
       "      <th>awesomeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000B049F5B33CD310EB1AB236E20191</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Even tho I love this album, I am having proble...</td>\n",
       "      <td>I LOVE NANCY THE BEAUTIFUL LEGEND, NANCY WILSO...</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.738333</td>\n",
       "      <td>0.037333</td>\n",
       "      <td>0.040262</td>\n",
       "      <td>0.038083</td>\n",
       "      <td>0.027154</td>\n",
       "      <td>0.471333</td>\n",
       "      <td>0.528667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420497</td>\n",
       "      <td>0.420497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018184A9EC4D270219A296B2580303</td>\n",
       "      <td>14</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>I have been a fan of GU's releases since i can...</td>\n",
       "      <td>One of GU's best The King of Progressive House...</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.805214</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.072115</td>\n",
       "      <td>0.077152</td>\n",
       "      <td>0.033816</td>\n",
       "      <td>0.192786</td>\n",
       "      <td>0.783786</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.217996</td>\n",
       "      <td>0.268650</td>\n",
       "      <td>0.087929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000281A9CAC43FF1F335726A390636DA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I made the mistake buying this album after lis...</td>\n",
       "      <td>Bad Business</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00030884DF109F325638A6BFD5B13CFF</td>\n",
       "      <td>20</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>Wow!  A must hear! Bob Marley at his best. Wha...</td>\n",
       "      <td>Maybe the Greatest Live Reggae Album Ever Bob ...</td>\n",
       "      <td>0.304300</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.021650</td>\n",
       "      <td>0.249407</td>\n",
       "      <td>0.237702</td>\n",
       "      <td>0.043391</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.717150</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.330916</td>\n",
       "      <td>0.325690</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000325BA25966B5FC701D5D2B5DBA4E0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Soft, melodic notes dot moving waves of ethere...</td>\n",
       "      <td>Light Notes Robin Miller/Transcendence relaxin...</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.052943</td>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>0.079333</td>\n",
       "      <td>0.780333</td>\n",
       "      <td>0.140333</td>\n",
       "      <td>0.137409</td>\n",
       "      <td>0.380474</td>\n",
       "      <td>0.243064</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71538</th>\n",
       "      <td>FFFDD3C72D23AF858D6E0ED92612370D</td>\n",
       "      <td>41</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>The bright yellow case on Kiss' sophomore albu...</td>\n",
       "      <td>'Cause baby's got the feeling, baby wants a sh...</td>\n",
       "      <td>0.280927</td>\n",
       "      <td>0.652244</td>\n",
       "      <td>0.066780</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>0.177106</td>\n",
       "      <td>0.044662</td>\n",
       "      <td>0.308610</td>\n",
       "      <td>0.618341</td>\n",
       "      <td>0.073049</td>\n",
       "      <td>0.307788</td>\n",
       "      <td>0.304994</td>\n",
       "      <td>0.155345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71539</th>\n",
       "      <td>FFFDDE284A73B29B320381487EC7DE9E</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>I picked up this CD for about $6 US and I have...</td>\n",
       "      <td>A leisurely stroll through the country AN HONE...</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.651750</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.311821</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.023580</td>\n",
       "      <td>0.305750</td>\n",
       "      <td>0.522250</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.353175</td>\n",
       "      <td>0.320673</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71540</th>\n",
       "      <td>FFFEB3EE2372807964F024707D50FB21</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Pop music has a short memory, but Kevin Roland...</td>\n",
       "      <td>A strong comeback by a troubled artist The 4th...</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.741500</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.127279</td>\n",
       "      <td>0.115258</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>0.251023</td>\n",
       "      <td>0.478711</td>\n",
       "      <td>0.228395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71541</th>\n",
       "      <td>FFFF4545AB232D81D0F9B208388BB7AA</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>I ordered the album as soon as I stumbled it, ...</td>\n",
       "      <td>EXCELLENT concept album, but shouldn't be the ...</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.775800</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.065238</td>\n",
       "      <td>0.085083</td>\n",
       "      <td>0.043895</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.667600</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.068813</td>\n",
       "      <td>0.140538</td>\n",
       "      <td>0.130139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71542</th>\n",
       "      <td>FFFF5A3D9CB0B40FF0FE6B95F05D26FE</td>\n",
       "      <td>23</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>Opinions are funny things. Everyone has differ...</td>\n",
       "      <td>MAKE YOUR OWN DECISION few good songs Worse th...</td>\n",
       "      <td>0.145739</td>\n",
       "      <td>0.739304</td>\n",
       "      <td>0.114870</td>\n",
       "      <td>0.083267</td>\n",
       "      <td>0.095374</td>\n",
       "      <td>0.088431</td>\n",
       "      <td>0.235565</td>\n",
       "      <td>0.571391</td>\n",
       "      <td>0.193043</td>\n",
       "      <td>0.342699</td>\n",
       "      <td>0.385184</td>\n",
       "      <td>0.337207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71543 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   asin  numReviews  percentVerified  \\\n",
       "0      0000B049F5B33CD310EB1AB236E20191           3         0.666667   \n",
       "1      00018184A9EC4D270219A296B2580303          14         0.071429   \n",
       "2      000281A9CAC43FF1F335726A390636DA           1         0.000000   \n",
       "3      00030884DF109F325638A6BFD5B13CFF          20         0.550000   \n",
       "4      000325BA25966B5FC701D5D2B5DBA4E0           3         1.000000   \n",
       "...                                 ...         ...              ...   \n",
       "71538  FFFDD3C72D23AF858D6E0ED92612370D          41         0.341463   \n",
       "71539  FFFDDE284A73B29B320381487EC7DE9E           4         0.500000   \n",
       "71540  FFFEB3EE2372807964F024707D50FB21           2         1.000000   \n",
       "71541  FFFF4545AB232D81D0F9B208388BB7AA           5         0.600000   \n",
       "71542  FFFF5A3D9CB0B40FF0FE6B95F05D26FE          23         0.043478   \n",
       "\n",
       "                                              reviewText  \\\n",
       "0      Even tho I love this album, I am having proble...   \n",
       "1      I have been a fan of GU's releases since i can...   \n",
       "2      I made the mistake buying this album after lis...   \n",
       "3      Wow!  A must hear! Bob Marley at his best. Wha...   \n",
       "4      Soft, melodic notes dot moving waves of ethere...   \n",
       "...                                                  ...   \n",
       "71538  The bright yellow case on Kiss' sophomore albu...   \n",
       "71539  I picked up this CD for about $6 US and I have...   \n",
       "71540  Pop music has a short memory, but Kevin Roland...   \n",
       "71541  I ordered the album as soon as I stumbled it, ...   \n",
       "71542  Opinions are funny things. Everyone has differ...   \n",
       "\n",
       "                                             summaryText  reviewPosMean  \\\n",
       "0      I LOVE NANCY THE BEAUTIFUL LEGEND, NANCY WILSO...       0.224000   \n",
       "1      One of GU's best The King of Progressive House...       0.141500   \n",
       "2                                           Bad Business       0.082000   \n",
       "3      Maybe the Greatest Live Reggae Album Ever Bob ...       0.304300   \n",
       "4      Light Notes Robin Miller/Transcendence relaxin...       0.266000   \n",
       "...                                                  ...            ...   \n",
       "71538  'Cause baby's got the feeling, baby wants a sh...       0.280927   \n",
       "71539  A leisurely stroll through the country AN HONE...       0.331250   \n",
       "71540  A strong comeback by a troubled artist The 4th...       0.234000   \n",
       "71541  EXCELLENT concept album, but shouldn't be the ...       0.164000   \n",
       "71542  MAKE YOUR OWN DECISION few good songs Worse th...       0.145739   \n",
       "\n",
       "       reviewNeuMean  reviewNegMean  reviewPosStDev  reviewNeuStDev  \\\n",
       "0           0.738333       0.037333        0.040262        0.038083   \n",
       "1           0.805214       0.053500        0.072115        0.077152   \n",
       "2           0.856000       0.062000        0.000000        0.000000   \n",
       "3           0.674100       0.021650        0.249407        0.237702   \n",
       "4           0.726667       0.007333        0.052943        0.047078   \n",
       "...              ...            ...             ...             ...   \n",
       "71538       0.652244       0.066780        0.197260        0.177106   \n",
       "71539       0.651750       0.017000        0.311821        0.305770   \n",
       "71540       0.741500       0.024500        0.127279        0.115258   \n",
       "71541       0.775800       0.060400        0.065238        0.085083   \n",
       "71542       0.739304       0.114870        0.083267        0.095374   \n",
       "\n",
       "       reviewNegStDev  summaryPosMean  summaryNeuMean  summaryNegMean  \\\n",
       "0            0.027154        0.471333        0.528667        0.000000   \n",
       "1            0.033816        0.192786        0.783786        0.023500   \n",
       "2            0.000000        0.000000        0.222000        0.778000   \n",
       "3            0.043391        0.274500        0.717150        0.008350   \n",
       "4            0.012702        0.079333        0.780333        0.140333   \n",
       "...               ...             ...             ...             ...   \n",
       "71538        0.044662        0.308610        0.618341        0.073049   \n",
       "71539        0.023580        0.305750        0.522250        0.172000   \n",
       "71540        0.012021        0.177500        0.661500        0.161500   \n",
       "71541        0.043895        0.274200        0.667600        0.058200   \n",
       "71542        0.088431        0.235565        0.571391        0.193043   \n",
       "\n",
       "       summaryPosStDev  summaryNeuStDev  summaryNegStDev  awesomeness  \n",
       "0             0.420497         0.420497         0.000000            1  \n",
       "1             0.217996         0.268650         0.087929            0  \n",
       "2             0.000000         0.000000         0.000000            0  \n",
       "3             0.330916         0.325690         0.037342            1  \n",
       "4             0.137409         0.380474         0.243064            0  \n",
       "...                ...              ...              ...          ...  \n",
       "71538         0.307788         0.304994         0.155345            1  \n",
       "71539         0.353175         0.320673         0.344000            1  \n",
       "71540         0.251023         0.478711         0.228395            0  \n",
       "71541         0.068813         0.140538         0.130139            1  \n",
       "71542         0.342699         0.385184         0.337207            1  \n",
       "\n",
       "[71543 rows x 18 columns]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b51663c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "1529.3509283065796\n"
     ]
    }
   ],
   "source": [
    "review_test1 = data_preprocessing('../devided_dataset_v2/CDs_and_Vinyl/test1/', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "959afa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test_features = review_test1.filter(['numReviews', 'percentVerified', 'reviewText', 'summaryText', \\\n",
    "                                          'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                          'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                          'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "301404a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>numReviews</th>\n",
       "      <th>percentVerified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summaryText</th>\n",
       "      <th>reviewPosMean</th>\n",
       "      <th>reviewNeuMean</th>\n",
       "      <th>reviewNegMean</th>\n",
       "      <th>reviewPosStDev</th>\n",
       "      <th>reviewNeuStDev</th>\n",
       "      <th>reviewNegStDev</th>\n",
       "      <th>summaryPosMean</th>\n",
       "      <th>summaryNeuMean</th>\n",
       "      <th>summaryNegMean</th>\n",
       "      <th>summaryPosStDev</th>\n",
       "      <th>summaryNeuStDev</th>\n",
       "      <th>summaryNegStDev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000093D54374AFE4B358EA5FBCB5776E</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Remember, this was the year people were freaki...</td>\n",
       "      <td>1999 A Very Special Christmas Collection... bu...</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.125158</td>\n",
       "      <td>0.075660</td>\n",
       "      <td>0.049497</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000B049F5B33CD310EB1AB236E20191</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Nancy Wilson is at her sensual and hypnotic be...</td>\n",
       "      <td>Hypnotic vocal magic~Bravo Nancy Wison!!!</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00018184A9EC4D270219A296B2580303</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I do not remember the exact words on Nick Warr...</td>\n",
       "      <td>Out of body experience Digweed strikes again!!...</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.761800</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.109822</td>\n",
       "      <td>0.091267</td>\n",
       "      <td>0.037885</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.735200</td>\n",
       "      <td>0.129400</td>\n",
       "      <td>0.302764</td>\n",
       "      <td>0.362747</td>\n",
       "      <td>0.289347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000281A9CAC43FF1F335726A390636DA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Erick Sermon &amp; Parrish Smith team back up afte...</td>\n",
       "      <td>4.5 stars - Erick &amp; Parrish Making Dollars</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00030884DF109F325638A6BFD5B13CFF</td>\n",
       "      <td>7</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>Classic recordings of BM. I do think the guita...</td>\n",
       "      <td>Great tunes a classic recording that you shoul...</td>\n",
       "      <td>0.217571</td>\n",
       "      <td>0.747429</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.087844</td>\n",
       "      <td>0.089976</td>\n",
       "      <td>0.032914</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322305</td>\n",
       "      <td>0.322305</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59921</th>\n",
       "      <td>FFFDD7F280DCD3E03DFA3D1F34592107</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>Yes, my opinion is somewhat biased.  The year ...</td>\n",
       "      <td>The first vinyl I ever owned POP FUN, NOTHING ...</td>\n",
       "      <td>0.207750</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.066062</td>\n",
       "      <td>0.038197</td>\n",
       "      <td>0.058178</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59922</th>\n",
       "      <td>FFFDDE284A73B29B320381487EC7DE9E</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Abbado's eqrly cycle of the Beethoven symphoni...</td>\n",
       "      <td>Far too middle of the road - but Abbado would ...</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59923</th>\n",
       "      <td>FFFEB3EE2372807964F024707D50FB21</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4th album from English band responsible for \"C...</td>\n",
       "      <td>a subtle record of sweet British soul</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59924</th>\n",
       "      <td>FFFF4545AB232D81D0F9B208388BB7AA</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Job for a Cowboy went from a super popular dea...</td>\n",
       "      <td>The Finest Album Yet FANTASTIC!! Job for a cow...</td>\n",
       "      <td>0.307750</td>\n",
       "      <td>0.642250</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.264804</td>\n",
       "      <td>0.247725</td>\n",
       "      <td>0.062716</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432511</td>\n",
       "      <td>0.432511</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59925</th>\n",
       "      <td>FFFF5A3D9CB0B40FF0FE6B95F05D26FE</td>\n",
       "      <td>7</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>Like most of America, my first exposure to Goo...</td>\n",
       "      <td>Not nearly as catchy as the last album... Five...</td>\n",
       "      <td>0.283571</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.115286</td>\n",
       "      <td>0.255031</td>\n",
       "      <td>0.205740</td>\n",
       "      <td>0.129207</td>\n",
       "      <td>0.261571</td>\n",
       "      <td>0.603571</td>\n",
       "      <td>0.134714</td>\n",
       "      <td>0.346969</td>\n",
       "      <td>0.342603</td>\n",
       "      <td>0.171926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59926 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   asin  numReviews  percentVerified  \\\n",
       "0      000093D54374AFE4B358EA5FBCB5776E           2         0.000000   \n",
       "1      0000B049F5B33CD310EB1AB236E20191           1         0.000000   \n",
       "2      00018184A9EC4D270219A296B2580303           5         0.000000   \n",
       "3      000281A9CAC43FF1F335726A390636DA           1         0.000000   \n",
       "4      00030884DF109F325638A6BFD5B13CFF           7         0.571429   \n",
       "...                                 ...         ...              ...   \n",
       "59921  FFFDD7F280DCD3E03DFA3D1F34592107           4         0.250000   \n",
       "59922  FFFDDE284A73B29B320381487EC7DE9E           1         0.000000   \n",
       "59923  FFFEB3EE2372807964F024707D50FB21           1         1.000000   \n",
       "59924  FFFF4545AB232D81D0F9B208388BB7AA           4         0.500000   \n",
       "59925  FFFF5A3D9CB0B40FF0FE6B95F05D26FE           7         0.142857   \n",
       "\n",
       "                                              reviewText  \\\n",
       "0      Remember, this was the year people were freaki...   \n",
       "1      Nancy Wilson is at her sensual and hypnotic be...   \n",
       "2      I do not remember the exact words on Nick Warr...   \n",
       "3      Erick Sermon & Parrish Smith team back up afte...   \n",
       "4      Classic recordings of BM. I do think the guita...   \n",
       "...                                                  ...   \n",
       "59921  Yes, my opinion is somewhat biased.  The year ...   \n",
       "59922  Abbado's eqrly cycle of the Beethoven symphoni...   \n",
       "59923  4th album from English band responsible for \"C...   \n",
       "59924  Job for a Cowboy went from a super popular dea...   \n",
       "59925  Like most of America, my first exposure to Goo...   \n",
       "\n",
       "                                             summaryText  reviewPosMean  \\\n",
       "0      1999 A Very Special Christmas Collection... bu...       0.135500   \n",
       "1              Hypnotic vocal magic~Bravo Nancy Wison!!!       0.209000   \n",
       "2      Out of body experience Digweed strikes again!!...       0.188600   \n",
       "3             4.5 stars - Erick & Parrish Making Dollars       0.123000   \n",
       "4      Great tunes a classic recording that you shoul...       0.217571   \n",
       "...                                                  ...            ...   \n",
       "59921  The first vinyl I ever owned POP FUN, NOTHING ...       0.207750   \n",
       "59922  Far too middle of the road - but Abbado would ...       0.151000   \n",
       "59923              a subtle record of sweet British soul       0.161000   \n",
       "59924  The Finest Album Yet FANTASTIC!! Job for a cow...       0.307750   \n",
       "59925  Not nearly as catchy as the last album... Five...       0.283571   \n",
       "\n",
       "       reviewNeuMean  reviewNegMean  reviewPosStDev  reviewNeuStDev  \\\n",
       "0           0.782500       0.082000        0.125158        0.075660   \n",
       "1           0.744000       0.047000        0.000000        0.000000   \n",
       "2           0.761800       0.049600        0.109822        0.091267   \n",
       "3           0.847000       0.030000        0.000000        0.000000   \n",
       "4           0.747429       0.035000        0.087844        0.089976   \n",
       "...              ...            ...             ...             ...   \n",
       "59921       0.742500       0.050000        0.066062        0.038197   \n",
       "59922       0.784000       0.065000        0.000000        0.000000   \n",
       "59923       0.822000       0.017000        0.000000        0.000000   \n",
       "59924       0.642250       0.050000        0.264804        0.247725   \n",
       "59925       0.601286       0.115286        0.255031        0.205740   \n",
       "\n",
       "       reviewNegStDev  summaryPosMean  summaryNeuMean  summaryNegMean  \\\n",
       "0            0.049497        0.125000        0.875000        0.000000   \n",
       "1            0.000000        0.000000        1.000000        0.000000   \n",
       "2            0.037885        0.135400        0.735200        0.129400   \n",
       "3            0.000000        0.000000        1.000000        0.000000   \n",
       "4            0.032914        0.410000        0.590000        0.000000   \n",
       "...               ...             ...             ...             ...   \n",
       "59921        0.058178        0.131000        0.869000        0.000000   \n",
       "59922        0.000000        0.278000        0.722000        0.000000   \n",
       "59923        0.000000        0.375000        0.625000        0.000000   \n",
       "59924        0.062716        0.539000        0.461000        0.000000   \n",
       "59925        0.129207        0.261571        0.603571        0.134714   \n",
       "\n",
       "       summaryPosStDev  summaryNeuStDev  summaryNegStDev  \n",
       "0             0.176777         0.176777         0.000000  \n",
       "1             0.000000         0.000000         0.000000  \n",
       "2             0.302764         0.362747         0.289347  \n",
       "3             0.000000         0.000000         0.000000  \n",
       "4             0.322305         0.322305         0.000000  \n",
       "...                ...              ...              ...  \n",
       "59921         0.262000         0.262000         0.000000  \n",
       "59922         0.000000         0.000000         0.000000  \n",
       "59923         0.000000         0.000000         0.000000  \n",
       "59924         0.432511         0.432511         0.000000  \n",
       "59925         0.346969         0.342603         0.171926  \n",
       "\n",
       "[59926 rows x 17 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "928614e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_ground_truths = pd.read_json('CDs_and_Vinyl_test1_labels.json')\n",
    "test1_ground_truths.sort_values('asin', inplace=True)\n",
    "y_actual = test1_ground_truths['awesomeness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "473a32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_transformer = Pipeline(\n",
    "    steps = [('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer())]\n",
    ")\n",
    "wordbagger = ColumnTransformer(\n",
    "    transformers=[(\"rev\", string_transformer, 'reviewText'), \n",
    "                  (\"sum\", string_transformer, 'summaryText')]\n",
    "    , remainder='passthrough'\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps = [(\"wordbag\",  wordbagger),\n",
    "                        (\"scale\", MaxAbsScaler()),\n",
    "                        ('varremove', VarianceThreshold()),\n",
    "                        ('select', SelectPercentile(percentile=100)),\n",
    "                        ('classify', LogisticRegression(solver=\"newton-cg\", C=0.05, max_iter = 500, n_jobs=-1))])\n",
    "                     #    VotingClassifier(estimators=[('RF', RandomForestClassifier(criterion=\"entropy\", max_depth=200, max_features=\"sqrt\")), \n",
    "                      #                                ('mN', MultinomialNB(alpha=0.001)), \n",
    "                       #                               ('LR', ], voting='soft'))])\n",
    "#x = review_group_df.filter(['numReviews', 'percentVerified', 'reviewText', 'summaryText'])\n",
    "#clf = LogisticRegression(tol = 0.001, max_iter = 150)\n",
    "#clf.fit(x,y)\n",
    "review_features = review_group_df.filter(['numReviews', 'percentVerified', 'reviewText', 'summaryText', \\\n",
    "                                          'reviewPosMean', 'reviewNeuMean', 'reviewNegMean', 'reviewPosStDev', \\\n",
    "                                          'reviewNeuStDev', 'reviewNegStDev', 'summaryPosMean', 'summaryNeuMean', \\\n",
    "                                          'summaryNegMean', 'summaryPosStDev','summaryNeuStDev','summaryNegStDev'])\n",
    "y = review_group_df.filter(['awesomeness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "dfaa95f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6367596026594334\n",
      "3.3495827198028563\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf.fit(review_features, np.ravel(y))\n",
    "y_predictions_test1 = clf.predict(review_test_features)\n",
    "f1 = f1_score(y_actual, y_predictions_test1, average='macro')\n",
    "print(f1)\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f6ed8c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('wordbag',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('rev',\n",
      "                                                  Pipeline(steps=[('vect',\n",
      "                                                                   CountVectorizer()),\n",
      "                                                                  ('tfidf',\n",
      "                                                                   TfidfTransformer())]),\n",
      "                                                  'reviewText'),\n",
      "                                                 ('sum',\n",
      "                                                  Pipeline(steps=[('vect',\n",
      "                                                                   CountVectorizer()),\n",
      "                                                                  ('tfidf',\n",
      "                                                                   TfidfTransformer())]),\n",
      "                                                  'summaryText')])),\n",
      "                ('scale', MaxAbsScaler()),\n",
      "                ('select', SelectPercentile(percentile=100)),\n",
      "                ('classify',\n",
      "                 LogisticRegression(C=0.05, max_iter=500, n_jobs=-1,\n",
      "                                    solver='newton-cg'))])\n"
     ]
    }
   ],
   "source": [
    "fileObj = open('classifier.obj', 'wb')\n",
    "pickle.dump(clf, fileObj)\n",
    "fileObj.close()\n",
    "fileObj = open('classifier.obj', 'rb')\n",
    "clf_loaded = pickle.load(fileObj)\n",
    "fileObj.close()\n",
    "print(clf_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "55512d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I started at 1684096293.8638282\n",
      "4.271603830655416\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"I started at \" + str(start))\n",
    "# this runs the k-fold cross-validation automatically?\n",
    "cv10_results = cross_validate(clf, review_features, np.ravel(y), cv=10, n_jobs = -1, scoring = ['f1_macro', 'precision', 'recall'])\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9f2d2011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6303313201814787\n",
      "0.6408943239310696\n",
      "0.7153122457740535\n"
     ]
    }
   ],
   "source": [
    "#print(cv10_results)\n",
    "#print(cv10_results)\n",
    "print(statistics.mean(cv10_results['test_f1_macro']))\n",
    "#print(statistics.stdev(cv10_results['test_f1_macro']))\n",
    "print(statistics.mean(cv10_results['test_precision']))\n",
    "#print(statistics.stdev(cv10_results['test_precision']))\n",
    "print(statistics.mean(cv10_results['test_recall']))\n",
    "#print(statistics.stdev(cv10_results['test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f9117d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[    0     1     2 ... 71540 71541 71542]\n",
      "  Test:  index=[    4    20    22 ... 71508 71520 71523]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 109762 features per sample; expecting 115607",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13680/463239938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mlog_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_selected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_proc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;31m#print(x_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#print(y_train.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0m\u001b[0;32m    289\u001b[0m                              % (X.shape[1], n_features))\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 109762 features per sample; expecting 115607"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(review_features)):\n",
    "    start = time.time()\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "    x_train = review_features.loc[train_index, :]\n",
    "    x_test = review_features.loc[test_index, :]\n",
    "    y_train = np.ravel(y.loc[train_index, :])\n",
    "    y_test = np.ravel(y.loc[test_index,:])\n",
    "    \n",
    "    x_train_proc = preprocessor.fit_transform(x_train)\n",
    "    x_test_proc = preprocessor.fit_transform(x_test)\n",
    "    \n",
    "    #x_train_features = wordbagger.fit_transform(x_train)\n",
    "    #x_test_features = wordbagger.fit_transform(x_test)\n",
    "    #scaler = preprocessing.MaxAbsScaler().fit(x_train_features)\n",
    "    #x_train_scaled = scaler.transform(x_train_features)\n",
    "    #print(type(y_train))\n",
    "    #print(x_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    #clf = Pipeline(steps = [(\"preprocess\", preprocessor), ('classifier', LogisticRegression()) ])\n",
    "    x_train_selected = select_clf.fit_transform(x_train_proc, y_train)\n",
    "    x_test_selected = select_clf.fit_transform(x_test_proc, y_test)\n",
    "    log_clf.fit(x_train_selected, y_train)\n",
    "    \n",
    "    y_pred = log_clf.predict(x_test_proc)\n",
    "    #print(x_train)\n",
    "    #print(y_train.shape)\n",
    "    #X_trans = preprocessor.fit_transform(x_train)\n",
    "    #print(x_train.shape)\n",
    "    print(\"model score: %.3f\" % f1_score(y_test, y_pred, 'macro'))\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "64b1e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select_clf = SelectFromModel(threshold = \"1.5*mean\", \\\n",
    " #                            estimator=LogisticRegression(solver=\"newton-cg\", C=0.05, max_iter = 500, n_jobs=-1))\n",
    "#select_clf = SelectFromModel(threshold = \"mean\", \\\n",
    "#                             estimator= MultinomialNB())\n",
    "\n",
    "#log_clf = LogisticRegression(solver=\"newton-cg\", C=0.05, max_iter = 500, n_jobs=-1)\n",
    "\n",
    "#random_forest_clf = RandomForestClassifier(max_depth = 150, n_jobs = -1)\n",
    "#boosted_clf = GradientBoostingClassifier()\n",
    "#just_clf.fit(processed_features, y)\n",
    "\n",
    "#voting_clf = VotingClassifier(estimators=[('RF', random_forest_clf), ('mN', multi_clf), ('LR', log_clf)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7a6713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectp_clf = SelectPercentile(score_func=chi2, percentile=14)\n",
    "multi_clf = MultinomialNB(alpha = 0.000001)\n",
    "var_processed_features = VarianceThreshold().fit_transform(processed_features)\n",
    "more_processed_features = selectp_clf.fit_transform(var_processed_features, np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d13e321a-2bf9-47f3-a4a6-c9dd1e4aec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm starting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.023726880550385\n"
     ]
    }
   ],
   "source": [
    "solvers = ['newton-cg']\n",
    "jobs = [-1]\n",
    "maxiter = [100, 500, 1000]\n",
    "c = [5, 2, 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01]\n",
    "param_grid={'classifier__C':c, 'classifier__solver':solvers, 'classifier__n_jobs':jobs, 'classifier__max_iter':maxiter}\n",
    "grid_clf = GridSearchCV(clf, param_grid, n_jobs=-1)\n",
    "start = time.time()\n",
    "print(\"I'm starting!\")\n",
    "grid_clf.fit(review_features, np.ravel(y))\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cea6b793-832b-4a98-bb2b-95193533a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_group_df.to_json(r'../devided_dataset_v2/CDs_and_Vinyl/train/cleaned_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "562fb612-31e0-43a7-9a47-cca8d896d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__max_iter</th>\n",
       "      <th>param_classifier__n_jobs</th>\n",
       "      <th>param_classifier__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.902761</td>\n",
       "      <td>3.981755</td>\n",
       "      <td>22.498697</td>\n",
       "      <td>0.715377</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.581173</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.586735</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263.848393</td>\n",
       "      <td>8.369911</td>\n",
       "      <td>22.481720</td>\n",
       "      <td>1.092433</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__max_iter': 5...</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.581173</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.586735</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258.521113</td>\n",
       "      <td>0.950623</td>\n",
       "      <td>21.237789</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.581173</td>\n",
       "      <td>0.590817</td>\n",
       "      <td>0.586735</td>\n",
       "      <td>0.590369</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>242.788301</td>\n",
       "      <td>6.899337</td>\n",
       "      <td>26.501668</td>\n",
       "      <td>2.905537</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.587253</td>\n",
       "      <td>0.587882</td>\n",
       "      <td>0.597666</td>\n",
       "      <td>0.594353</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.592958</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>245.064487</td>\n",
       "      <td>12.992323</td>\n",
       "      <td>22.550827</td>\n",
       "      <td>1.717254</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__max_iter': 5...</td>\n",
       "      <td>0.587253</td>\n",
       "      <td>0.587882</td>\n",
       "      <td>0.597666</td>\n",
       "      <td>0.594353</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.592958</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>230.603351</td>\n",
       "      <td>2.580260</td>\n",
       "      <td>22.640238</td>\n",
       "      <td>0.726609</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.587253</td>\n",
       "      <td>0.587882</td>\n",
       "      <td>0.597666</td>\n",
       "      <td>0.594353</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.592958</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>217.788828</td>\n",
       "      <td>4.732246</td>\n",
       "      <td>23.282278</td>\n",
       "      <td>1.899143</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.596478</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.604697</td>\n",
       "      <td>0.600241</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>219.208971</td>\n",
       "      <td>2.917052</td>\n",
       "      <td>21.258987</td>\n",
       "      <td>0.611024</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__max_iter': 5...</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.596478</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.604697</td>\n",
       "      <td>0.600241</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>214.697790</td>\n",
       "      <td>8.029622</td>\n",
       "      <td>23.524169</td>\n",
       "      <td>2.573233</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__max_iter': 1...</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.596478</td>\n",
       "      <td>0.602767</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.604697</td>\n",
       "      <td>0.600241</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202.574223</td>\n",
       "      <td>15.012335</td>\n",
       "      <td>22.778429</td>\n",
       "      <td>3.025260</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__max_iter':...</td>\n",
       "      <td>0.603816</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.613573</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>203.391861</td>\n",
       "      <td>2.496271</td>\n",
       "      <td>21.045616</td>\n",
       "      <td>0.183373</td>\n",
       "      <td>0.5</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__max_iter':...</td>\n",
       "      <td>0.603816</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.613573</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>193.466252</td>\n",
       "      <td>13.984599</td>\n",
       "      <td>22.463085</td>\n",
       "      <td>2.696157</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__max_iter':...</td>\n",
       "      <td>0.603816</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.613573</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>182.137830</td>\n",
       "      <td>19.212568</td>\n",
       "      <td>21.655008</td>\n",
       "      <td>1.693563</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__max_iter':...</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.626669</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>183.678528</td>\n",
       "      <td>2.561745</td>\n",
       "      <td>21.394557</td>\n",
       "      <td>0.871452</td>\n",
       "      <td>0.2</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__max_iter':...</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.626669</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>175.311657</td>\n",
       "      <td>12.944425</td>\n",
       "      <td>21.336044</td>\n",
       "      <td>1.767870</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__max_iter':...</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>0.626669</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>178.296248</td>\n",
       "      <td>2.926505</td>\n",
       "      <td>21.630373</td>\n",
       "      <td>0.291683</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__max_iter':...</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.625690</td>\n",
       "      <td>0.636732</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.631662</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>172.294253</td>\n",
       "      <td>10.734698</td>\n",
       "      <td>22.667302</td>\n",
       "      <td>2.671143</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__max_iter':...</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.625690</td>\n",
       "      <td>0.636732</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.631662</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>173.066506</td>\n",
       "      <td>7.797931</td>\n",
       "      <td>22.042321</td>\n",
       "      <td>2.201104</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__max_iter':...</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.625690</td>\n",
       "      <td>0.636732</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>0.631662</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>160.185623</td>\n",
       "      <td>6.154684</td>\n",
       "      <td>22.120637</td>\n",
       "      <td>1.715451</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.05, 'classifier__max_iter'...</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>0.638454</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.635814</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>154.716878</td>\n",
       "      <td>4.048059</td>\n",
       "      <td>23.099909</td>\n",
       "      <td>2.722425</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.05, 'classifier__max_iter'...</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>0.638454</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.635814</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>160.146975</td>\n",
       "      <td>4.332940</td>\n",
       "      <td>21.622918</td>\n",
       "      <td>0.544349</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.05, 'classifier__max_iter'...</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.639248</td>\n",
       "      <td>0.638454</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.635814</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>141.816327</td>\n",
       "      <td>3.315259</td>\n",
       "      <td>23.385035</td>\n",
       "      <td>1.704255</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.02, 'classifier__max_iter'...</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>140.778459</td>\n",
       "      <td>8.244477</td>\n",
       "      <td>23.098364</td>\n",
       "      <td>3.046290</td>\n",
       "      <td>0.02</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.02, 'classifier__max_iter'...</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>144.068076</td>\n",
       "      <td>2.543079</td>\n",
       "      <td>21.405944</td>\n",
       "      <td>0.622739</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.02, 'classifier__max_iter'...</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.638174</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.634723</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>140.789716</td>\n",
       "      <td>5.046037</td>\n",
       "      <td>23.505679</td>\n",
       "      <td>1.898376</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__max_iter'...</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>0.622405</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.632793</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>141.560911</td>\n",
       "      <td>12.842808</td>\n",
       "      <td>20.430560</td>\n",
       "      <td>1.227738</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__max_iter'...</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>0.622405</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.632793</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>109.830957</td>\n",
       "      <td>4.216425</td>\n",
       "      <td>12.254573</td>\n",
       "      <td>3.053849</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>-1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__max_iter'...</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>0.622405</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.632793</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      265.902761      3.981755        22.498697        0.715377   \n",
       "1      263.848393      8.369911        22.481720        1.092433   \n",
       "2      258.521113      0.950623        21.237789        0.196200   \n",
       "3      242.788301      6.899337        26.501668        2.905537   \n",
       "4      245.064487     12.992323        22.550827        1.717254   \n",
       "5      230.603351      2.580260        22.640238        0.726609   \n",
       "6      217.788828      4.732246        23.282278        1.899143   \n",
       "7      219.208971      2.917052        21.258987        0.611024   \n",
       "8      214.697790      8.029622        23.524169        2.573233   \n",
       "9      202.574223     15.012335        22.778429        3.025260   \n",
       "10     203.391861      2.496271        21.045616        0.183373   \n",
       "11     193.466252     13.984599        22.463085        2.696157   \n",
       "12     182.137830     19.212568        21.655008        1.693563   \n",
       "13     183.678528      2.561745        21.394557        0.871452   \n",
       "14     175.311657     12.944425        21.336044        1.767870   \n",
       "15     178.296248      2.926505        21.630373        0.291683   \n",
       "16     172.294253     10.734698        22.667302        2.671143   \n",
       "17     173.066506      7.797931        22.042321        2.201104   \n",
       "18     160.185623      6.154684        22.120637        1.715451   \n",
       "19     154.716878      4.048059        23.099909        2.722425   \n",
       "20     160.146975      4.332940        21.622918        0.544349   \n",
       "21     141.816327      3.315259        23.385035        1.704255   \n",
       "22     140.778459      8.244477        23.098364        3.046290   \n",
       "23     144.068076      2.543079        21.405944        0.622739   \n",
       "24     140.789716      5.046037        23.505679        1.898376   \n",
       "25     141.560911     12.842808        20.430560        1.227738   \n",
       "26     109.830957      4.216425        12.254573        3.053849   \n",
       "\n",
       "   param_classifier__C param_classifier__max_iter param_classifier__n_jobs  \\\n",
       "0                    5                        100                       -1   \n",
       "1                    5                        500                       -1   \n",
       "2                    5                       1000                       -1   \n",
       "3                    2                        100                       -1   \n",
       "4                    2                        500                       -1   \n",
       "5                    2                       1000                       -1   \n",
       "6                    1                        100                       -1   \n",
       "7                    1                        500                       -1   \n",
       "8                    1                       1000                       -1   \n",
       "9                  0.5                        100                       -1   \n",
       "10                 0.5                        500                       -1   \n",
       "11                 0.5                       1000                       -1   \n",
       "12                 0.2                        100                       -1   \n",
       "13                 0.2                        500                       -1   \n",
       "14                 0.2                       1000                       -1   \n",
       "15                 0.1                        100                       -1   \n",
       "16                 0.1                        500                       -1   \n",
       "17                 0.1                       1000                       -1   \n",
       "18                0.05                        100                       -1   \n",
       "19                0.05                        500                       -1   \n",
       "20                0.05                       1000                       -1   \n",
       "21                0.02                        100                       -1   \n",
       "22                0.02                        500                       -1   \n",
       "23                0.02                       1000                       -1   \n",
       "24                0.01                        100                       -1   \n",
       "25                0.01                        500                       -1   \n",
       "26                0.01                       1000                       -1   \n",
       "\n",
       "   param_classifier__solver  \\\n",
       "0                 newton-cg   \n",
       "1                 newton-cg   \n",
       "2                 newton-cg   \n",
       "3                 newton-cg   \n",
       "4                 newton-cg   \n",
       "5                 newton-cg   \n",
       "6                 newton-cg   \n",
       "7                 newton-cg   \n",
       "8                 newton-cg   \n",
       "9                 newton-cg   \n",
       "10                newton-cg   \n",
       "11                newton-cg   \n",
       "12                newton-cg   \n",
       "13                newton-cg   \n",
       "14                newton-cg   \n",
       "15                newton-cg   \n",
       "16                newton-cg   \n",
       "17                newton-cg   \n",
       "18                newton-cg   \n",
       "19                newton-cg   \n",
       "20                newton-cg   \n",
       "21                newton-cg   \n",
       "22                newton-cg   \n",
       "23                newton-cg   \n",
       "24                newton-cg   \n",
       "25                newton-cg   \n",
       "26                newton-cg   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'classifier__C': 5, 'classifier__max_iter': 1...           0.579356   \n",
       "1   {'classifier__C': 5, 'classifier__max_iter': 5...           0.579356   \n",
       "2   {'classifier__C': 5, 'classifier__max_iter': 1...           0.579356   \n",
       "3   {'classifier__C': 2, 'classifier__max_iter': 1...           0.587253   \n",
       "4   {'classifier__C': 2, 'classifier__max_iter': 5...           0.587253   \n",
       "5   {'classifier__C': 2, 'classifier__max_iter': 1...           0.587253   \n",
       "6   {'classifier__C': 1, 'classifier__max_iter': 1...           0.595220   \n",
       "7   {'classifier__C': 1, 'classifier__max_iter': 5...           0.595220   \n",
       "8   {'classifier__C': 1, 'classifier__max_iter': 1...           0.595220   \n",
       "9   {'classifier__C': 0.5, 'classifier__max_iter':...           0.603816   \n",
       "10  {'classifier__C': 0.5, 'classifier__max_iter':...           0.603816   \n",
       "11  {'classifier__C': 0.5, 'classifier__max_iter':...           0.603816   \n",
       "12  {'classifier__C': 0.2, 'classifier__max_iter':...           0.619890   \n",
       "13  {'classifier__C': 0.2, 'classifier__max_iter':...           0.619890   \n",
       "14  {'classifier__C': 0.2, 'classifier__max_iter':...           0.619890   \n",
       "15  {'classifier__C': 0.1, 'classifier__max_iter':...           0.629045   \n",
       "16  {'classifier__C': 0.1, 'classifier__max_iter':...           0.629045   \n",
       "17  {'classifier__C': 0.1, 'classifier__max_iter':...           0.629045   \n",
       "18  {'classifier__C': 0.05, 'classifier__max_iter'...           0.632399   \n",
       "19  {'classifier__C': 0.05, 'classifier__max_iter'...           0.632399   \n",
       "20  {'classifier__C': 0.05, 'classifier__max_iter'...           0.632399   \n",
       "21  {'classifier__C': 0.02, 'classifier__max_iter'...           0.630372   \n",
       "22  {'classifier__C': 0.02, 'classifier__max_iter'...           0.630372   \n",
       "23  {'classifier__C': 0.02, 'classifier__max_iter'...           0.630372   \n",
       "24  {'classifier__C': 0.01, 'classifier__max_iter'...           0.627787   \n",
       "25  {'classifier__C': 0.01, 'classifier__max_iter'...           0.627787   \n",
       "26  {'classifier__C': 0.01, 'classifier__max_iter'...           0.627787   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.581173           0.590817           0.586735   \n",
       "1            0.581173           0.590817           0.586735   \n",
       "2            0.581173           0.590817           0.586735   \n",
       "3            0.587882           0.597666           0.594353   \n",
       "4            0.587882           0.597666           0.594353   \n",
       "5            0.587882           0.597666           0.594353   \n",
       "6            0.596478           0.602767           0.602041   \n",
       "7            0.596478           0.602767           0.602041   \n",
       "8            0.596478           0.602767           0.602041   \n",
       "9            0.608288           0.612831           0.613363   \n",
       "10           0.608288           0.612831           0.613363   \n",
       "11           0.608288           0.612831           0.613363   \n",
       "12           0.618632           0.626669           0.625245   \n",
       "13           0.618632           0.626669           0.625245   \n",
       "14           0.618632           0.626669           0.625245   \n",
       "15           0.625690           0.636732           0.632723   \n",
       "16           0.625690           0.636732           0.632723   \n",
       "17           0.625690           0.636732           0.632723   \n",
       "18           0.630163           0.639248           0.638454   \n",
       "19           0.630163           0.639248           0.638454   \n",
       "20           0.630163           0.639248           0.638454   \n",
       "21           0.627857           0.637291           0.638174   \n",
       "22           0.627857           0.637291           0.638174   \n",
       "23           0.627857           0.637291           0.638174   \n",
       "24           0.622405           0.631351           0.632793   \n",
       "25           0.622405           0.631351           0.632793   \n",
       "26           0.622405           0.631351           0.632793   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.590369         0.585690        0.004686               25  \n",
       "1            0.590369         0.585690        0.004686               25  \n",
       "2            0.590369         0.585690        0.004686               25  \n",
       "3            0.597638         0.592958        0.004568               22  \n",
       "4            0.597638         0.592958        0.004568               22  \n",
       "5            0.597638         0.592958        0.004568               22  \n",
       "6            0.604697         0.600241        0.003711               19  \n",
       "7            0.604697         0.600241        0.003711               19  \n",
       "8            0.604697         0.600241        0.003711               19  \n",
       "9            0.613573         0.610374        0.003810               16  \n",
       "10           0.613573         0.610374        0.003810               16  \n",
       "11           0.613573         0.610374        0.003810               16  \n",
       "12           0.625594         0.623206        0.003279               13  \n",
       "13           0.625594         0.623206        0.003279               13  \n",
       "14           0.625594         0.623206        0.003279               13  \n",
       "15           0.634121         0.631662        0.003883                7  \n",
       "16           0.634121         0.631662        0.003883                7  \n",
       "17           0.634121         0.631662        0.003883                7  \n",
       "18           0.638803         0.635814        0.003776                1  \n",
       "19           0.638803         0.635814        0.003776                1  \n",
       "20           0.638803         0.635814        0.003776                1  \n",
       "21           0.639922         0.634723        0.004725                4  \n",
       "22           0.639922         0.634723        0.004725                4  \n",
       "23           0.639922         0.634723        0.004725                4  \n",
       "24           0.633003         0.629468        0.003995               10  \n",
       "25           0.633003         0.629468        0.003995               10  \n",
       "26           0.633003         0.629468        0.003995               10  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(grid_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd032e-2505-4180-941e-aa55e34aa53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccd2b721",
   "metadata": {},
   "source": [
    "All the older stuff is below here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bb63e",
   "metadata": {},
   "source": [
    "NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a38ef6d-1722-4684-9d96-93b6e5790cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I am being handed a list of documents\", \"Each of these documents has several unique words\", \"The words will represent the class of each review\", \"I am also removing stopwords in order to make this make more sense\"]\n",
    "cleaned_corpus = [transform_document(doc) for doc in corpus]\n",
    "vocabulary = vocabulary_from_corpus(cleaned_corpus, True)\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), \n",
    "                 ('tfid', TfidfTransformer())]).fit(cleaned_corpus)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c8e5cb-dfb0-4602-b800-3f6cc25f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    file = open('en.txt')\n",
    "    stopwords = []\n",
    "    for line in file:\n",
    "        stopwords.append(line.rstrip())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c4e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_corpus(review_group):\n",
    "    stopwords = get_stopwords()\n",
    "    reviewTextSet = review_group['reviewText']\n",
    "    for index in review_group.index:\n",
    "        curr_parsed = nlp(reviewTextSet[index].lower())\n",
    "        doclist = []\n",
    "        for token in curr_parsed:\n",
    "            lemma = token.lemma_\n",
    "            if not(re.match(\"[a-z0-9]+\", lemma)):\n",
    "                continue\n",
    "            if lemma not in stopwords:\n",
    "                doclist.append(lemma)\n",
    "        reviewTextSet[index] = \" \".join(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9a82e873-6a79-4d55-9c06-94041197a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_document(doc, remove_stopwords = True):\n",
    "    #new_doc = \"\"\n",
    "    stopwords = get_stopwords() # is this slow?\n",
    "    parsed_text = nlp(doc) # is this slow\n",
    "    doclist = []\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            doclist.append(lemma) # this is less slow?\n",
    "    return \" \".join(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "944f9414-c74e-4287-8756-74e82d279fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(review_text, remove_stopwords = True):\n",
    "    word_bag = {}\n",
    "    stopwords = get_stopwords()\n",
    "    parsed_text = nlp(review_text)\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if re.match(\"[a-z0-9]+\", lemma) and (remove_stopwords == False or lemma not in stopwords):\n",
    "            if lemma in word_bag:\n",
    "                word_bag[lemma] += 1\n",
    "            else:\n",
    "                word_bag[lemma] = 1\n",
    "    return word_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c054d665-1ac0-4e05-9826-b96a2bc4bb43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'the', 'second', 'third', 'be', 'and', 'this', 'one', 'document']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vocabulary_from_corpus(corpus, remove_stopwords = True):\n",
    "    vocab_set = set()\n",
    "    for document in corpus:\n",
    "        word_bag = bag_of_words(document, remove_stopwords)\n",
    "        for word in word_bag.keys():\n",
    "            vocab_set.add(word)\n",
    "    return list(vocab_set)\n",
    "vocabulary_from_corpus(['this is the first document', 'this document is the second document', 'and this is the third one', 'is this the first document'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da656ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/product_training.json')\n",
    "review_df = pd.read_json('../devided_dataset_v2/CDs_and_Vinyl/train/review_training.json')\n",
    "#len(product_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098d8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "teststring = \"First, you need to preprocess the raw text data. This may involve tasks like tokenizing the text (i.e., splitting it into individual words), removing stopwords, stemming or lemmatizing the words, and converting the text into a numerical format that can be used as input for the model. Then, you need to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance.\"\n",
    "#teststring.lower()\n",
    "parsed = nlp(teststring.lower())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
